## 🙋🏻‍♀️ 스물한번째 비대면 수업

---

# 내용 정리

---

[LVM redhat pdf](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/pdf/logical_volume_manager_administration/Red_Hat_Enterprise_Linux-6-Logical_Volume_Manager_Administration-ko-KR.pdf)

■ LVM 작업 순서

System Partition ID 변경(fdisk CMD)	

-> PV 생성(pvcreate CMD)	

-> VG 생성(vgcreate CMD)	

**-> LV 생성(lvcreate CMD)**	:  디스크의 파티션

-> F/S 생성(mkfs CMD)	

-> 마운트(mount CMD, /etc/fstab)

[3] 논리 볼륨 생성(LV: Logical Volume)

논리 볼륨은 LVM에서 생성하는 파티션이므로 단일 파티션 또는 다중 파티션으로 나눌 수 있는데, 이때 논리 볼륨을 생성하는 방법은 용량 단위 또는 PE수를 통해 생성해 줄 수 있다.

[명령어 형식]

```bash
■ LV 생성
# lvcreate -L 10G vg1            /* LV 이름 자동 생성 : lvol#(용량 10G) */
# lvcreate -L 1500M -n lv1 vg1
# lvcreate -L 1500 -n lv1 vg1    /* 용량 단위가 없으면 MB */
# lvcreate -l 60%VG -n lv1 vg1
# lvcreate -l 100%FREE -n lv1 vg1

■ LV 삭제
# lvremove /dev/vg1/lv1

■ LV 이름 변경
# lvrename /dev/vg1/lv1 /dev/vg1/lv2
# lvrename vg1 lv1 lv2           /* vg1(lv1 -> lv2) */

■ LV 공간 늘리기/줄이기
# lvextend -L 12G /dev/vg1/lv1   /* lv1 용량을 12G로 맞춤 */
# lvextend -L +1G /dev/vg1/lv1   /* lv1 용량을 1G 만큼 추가 */
# lvextend -l +100%FREE /dev/vg1/lv1

# lvreduce -L -3G /dev/vg1/lv1
# lvreduce -l -3 /dev/vg1/lv1    /* lv1 용량을 3 LE(Logical Extend)만큼 감소 */

■ LV 정보 확인
# lvs 
# lvscan 
# lvdisplay
```

### [EX] LV 생성 및 삭제

```bash
① LV 생성 및 확인
# vgdisplay (# vgs)
--- Volume group ---
  VG Name               vg1
  System ID             
  Format                lvm2
  Metadata Areas        3
  Metadata Sequence No  5
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                3
  Act PV                3
  VG Size               2.98 GB
  PE Size               4.00 MB
  Total PE              762
  Alloc PE / Size       0 / 0   
  Free  PE / Size       762 / 2.98 GB
  VG UUID               NHG3Mo-boPO-5z8j-XBYb-K5zJ-vtuy-6wnejz
# lvcreate -L 1G -n lv1 vg1                /* -L : 용량 , -n : 이름 */
Logical volume "lv1" created
# lvcreate -L 1G -n lv2 vg1
Logical volume "lv2" created
# lvcreate -l 100%FREE -n lv3 vg1          /* 남은 용량 */
Logical volume "lv3" created
		[참고] -L 옵션으로 잘못 사용하는 경우
		# lvcreate -L 100%FREE -n lv3 vg3
		  Invalid argument 100%FREE
		  Error during parsing of command line.
# ls -l /dev/vg1/lv1
lrwxrwxrwx. 1 root root 7 Dec 22 09:53 /dev/vg1/lv1 -> ../dm-0
# ls -l /dev/mapper/vg1-lv1
lrwxrwxrwx. 1 root root 7 Dec 22 09:53 /dev/mapper/vg1-lv1 -> ../dm-0
# lvs
	LV   VG   Attr   LSize    Origin Snap%  Move Log Copy%  Convert
  lv1  vg1  -wi-a-    1.00G                                      
  lv2  vg1  -wi-a-    1.00G                                      
  lv3  vg1  -wi-a- 1000.00M
# lvscan
ACTIVE            '/dev/vg1/lv1' [1.00 GB] inherit
ACTIVE            '/dev/vg1/lv2' [1.00 GB] inherit
ACTIVE            '/dev/vg1/lv3' [1000.00 MB] inherit
# lvdisplay /dev/vg1/lv3  (# lvdisplay)
--- Logical volume ---
  LV Name                /dev/vg1/lv3
  VG Name                vg1
  LV UUID                CgQVDH-bxfR-qI21-RHd4-WJYz-CTsx-YVWr27
  LV Write Access        read/write
  LV Status              available
  # open                 0
  LV Size                1000.00 MB
  Current LE             250
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:2

② LV 생성 다른 방법 (LE의 개수로 생성하기)  관리 지점이 많아지면 장애 지점도 많아진다. 때문에 간소화한다.
# vgdisplay vg1 | grep -i "total pe"
  Total PE              765
# lvcreate -l 765 -n lv1 vg1
  Logical volume "lv1" created.
# lvs
  LV   VG  Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv1  vg1 -wi-a----- <2.99g
# lvdisplay
  --- Logical volume ---
  LV Path                /dev/vg1/lv1
  LV Name                lv1
  VG Name                vg1
  LV UUID                y9P8Kq-eRBt-8L5x-L6sa-PG6y-h818-yrHp7H
  LV Write Access        read/write
  LV Creation host, time server1.example.com, 2020-12-22 10:17:20 +0900
  LV Status              available
  # open                 0
  LV Size                <2.99 GiB
  Current LE             765
  Segments               3
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:0
# vgs
  VG  #PV #LV #SN Attr   VSize  VFree
  vg1   3   1   0 wz--n- <2.99g    0

② LV 삭제 및 확인
# lvremove /dev/vg1/lv3
Do you really want to remove active logical volume lv3? [y/n]: y
Logical volume "lv3" successfully removed
# lvs
LV   VG   Attr   LSize Origin Snap%  Move Log Copy%  Convert
  lv1  vg1  -wi-a- 1.00G                                      
  lv2  vg1  -wi-a- 1.00G
-> lv3 삭제
# vgdisplay | grep -i "total pe"
  Total PE              765
# vgdisplay | grep -i "pe"
  Open LV               0
  PE Size               4.00 MiB
  Total PE              765
  Alloc PE / Size       512 / 2.00 GiB
  Free  PE / Size       253 / 1012.00 MiB
# vgdisplay | grep -i "free  pe"
  Free  PE / Size       253 / 1012.00 MiB

③ LV 이름 변경 (장치 이름이 바뀐다. 이미 마운트 된 것 이름 변경시 위험할 수 있다)
# lvrename /dev/vg1/lv2 /dev/vg1/lv2_backup
Renamed "lv2" to "lv2_backup" in volume group "vg1"
# lvs
LV         VG   Attr   LSize Origin Snap%  Move Log Copy%  Convert
  lv1        vg1  -wi-a- 1.00G                                      
  lv2_backup vg1  -wi-a- 1.00G
# lvrename vg1 lv2_backup lv2   (# lvrename --help)
Renamed "lv2_backup" to "lv2" in volume group "vg1"
# lvs
LV   VG   Attr   LSize Origin Snap%  Move Log Copy%  Convert
  lv1  vg1  -wi-a- 1.00G                                      
  lv2  vg1  -wi-a- 1.00G

④ LV 용량 늘리기 및 확인 (파일시스템이 마운트가 되어 있는 상태에서는 추가 작업이 있다. 위험하다)
# vgs     (# vgdisplay | grep PE)
VG   #PV #LV #SN Attr   VSize VFree   
  vg1    3   2   0 wz--n- 2.98G 1000.00M
-> vg1 남은 공간 확인
# lvextend -L +500M /dev/vg1/lv1    (# lvextend -L 1500M /dev/vg1/lv1)
Extending logical volume lv1 to 1.49 GB
  Logical volume lv1 successfully resized
# lvs
LV   VG   Attr   LSize Origin Snap%  Move Log Copy%  Convert
  lv1  vg1  -wi-a- 1.49G                                      
  lv2  vg1  -wi-a- 1.00G
# lvextend -l 100%FREE /dev/vg1/lv2  /* 확장시에는 +추가해야됨 */
  New size given (128 extents) not larger than existing size (256 extents)
# vgs
  VG  #PV #LV #SN Attr   VSize  VFree
  vg1   3   2   0 wz--n- <2.99g 512.00m
# lvextend -l +100%FREE /dev/vg1/lv2
  Size of logical volume vg1/lv2 changed from 1.00 GiB (256 extents) to 1.50 GiB (384 extents).
  Logical volume vg1/lv2 successfully resized.
# lvs
  LV   VG  Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv1  vg1 -wi-a----- <1.49g
  lv2  vg1 -wi-a-----  1.50g

⑤ LV 크기 줄이기 (위험하다)
(주의) LV 크기를 줄이기 위해서는 백업 과정을 거친다.
# lvreduce -L -500M /dev/vg1/lv2
WARNING: Reducing active logical volume to 524.00 MB
  THIS MAY DESTROY YOUR DATA (filesystem etc.)
Do you really want to reduce lv2? [y/n]: y
  Reducing logical volume lv2 to 524.00 MB
  Logical volume lv2 successfully resized
# lvs
LV   VG   Attr   LSize   Origin Snap%  Move Log Copy%  Convert
  lv1  vg1  -wi-a-   1.49G                                      
  lv2  vg1  -wi-a- 524.00M

```

❗삭제시 주의점❗

반드시 작업 순서의 역순으로 작업을 해야한다.

LV삭제 → VG 삭제 → PV 삭제

■ LVM 작업 순서

System Partition ID 변경(fdisk CMD)	

-> PV 생성(pvcreate CMD)	

-> VG 생성(vgcreate CMD)	

**-> LV 생성(lvcreate CMD)**	

-> F/S 생성(mkfs CMD)	

-> 마운트(mount CMD, /etc/fstab)

[4] 논리 볼륨 ext4로 파일시스템 생성

논리 볼륨에 대해 ext4파일 시스템을 갖도록 mkfs.ext4로 포맷한다.

### [EX] 파일시스템 생성 실습

```bash
# lvs
LV   VG   Attr   LSize   Origin Snap%  Move Log Copy%  Convert
  lv1  vg1  -wi-a-   1.49G                                      
  lv2  vg1  -wi-a- 524.00M
# mkfs.ext4 /dev/vg1/lv1   (# mkfs -t ext4 /dev/vg1/lv1)
mke2fs 1.39 (29-May-2006)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
195072 inodes, 390144 blocks
19507 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=402653184
12 block groups
32768 blocks per group, 32768 fragments per group
16256 inodes per group
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912

Writing inode tables: done                            
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done

This filesystem will be automatically checked every 31 mounts or
180 days, whichever comes first.  Use tune2fs -c or -i to override.
# mkfs -t ext4 /dev/vg1/lv2
mke2fs 1.39 (29-May-2006)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
67200 inodes, 134144 blocks
6707 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=138412032
5 block groups
32768 blocks per group, 32768 fragments per group
13440 inodes per group
Superblock backups stored on blocks: 
        32768, 98304

Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done

This filesystem will be automatically checked every 31 mounts or
180 days, whichever comes first.  Use tune2fs -c or -i to override.

◎ lv2는 xfs로 작업
# mkfs.xfs /dev/vg1/lv2
meta-data=/dev/vg1/lv2           isize=512    agcount=4, agsize=32000 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=128000, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=1368, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none
```

■ LVM 작업 순서

System Partition ID 변경(fdisk CMD)	

-> PV 생성(pvcreate CMD)	

-> VG 생성(vgcreate CMD)	

**-> LV 생성(lvcreate CMD)**	

-> F/S 생성(mkfs CMD)	

-> 마운트(mount CMD, /etc/fstab)

[5] 마운트 작업

생성된 LV를 마운트 한다.

### [EX] 마운트 실습

```bash
# mkdir /lv1 /lv2 
# mount /dev/vg1/lv1 /lv1 
# mount /dev/vg1/lv2 /lv2 
# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1              17G  3.4G   12G  22% /
/dev/sda8             487M   11M  451M   3% /data1
/dev/sda7             487M   11M  451M   3% /data2
/dev/sda6             487M   11M  451M   3% /data3
/dev/sda5             487M   11M  451M   3% /data4
/dev/sda3             487M   11M  451M   3% /home
tmpfs                 506M     0  506M   0% /dev/shm
/dev/sdb1            1004M   49M  904M   6% /testmount
/dev/mapper/vg1-lv1   1.5G   35M  1.4G   3% /lv1
/dev/mapper/vg1-lv2   516M   17M  474M   4% /lv2

# vi /etc/fstab
LABEL=/                 /                       ext4    defaults        1 1
LABEL=/data1            /data1                  ext4    defaults        1 2
LABEL=/data2            /data2                  ext4    defaults        1 2
LABEL=/data3            /data3                  ext4    defaults        1 2
LABEL=/data4            /data4                  ext4    defaults        1 2
LABEL=/home             /home                   ext4    defaults        1 2
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
LABEL=SWAP-sda2         swap                    swap    defaults        0 0
#
# (1) DISK Configuration
#
#/dev/sdb1              /testmount              ext4    defaults        1 3
#LABEL=/testmount       /testmount              ext4    defaults        1 3
#
# (2) LVM Configuration
#
/dev/mapper/vg1-lv1     /lv1                    ext4    defaults        1 3
/dev/vg1/lv2            /lv2                    ext4    defaults        1 3
# umount /lv1 
# umount /lv2 
# mount  /lv1 
# mount  /lv2 

# mount
/dev/sda1 on / type ext4 (rw)
proc on /proc type proc (rw)
sysfs on /sys type sysfs (rw)
devpts on /dev/pts type devpts (rw,gid=5,mode=620)
..... (중략) .....
none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
none on /proc/fs/vmblock/mountPoint type vmblock (rw)
sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)
/dev/mapper/vg1-lv1 on /lv1 type ext4 (rw)
/dev/mapper/vg1-lv2 on /lv2 type ext4 (rw)
# df –h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1              17G  3.4G   12G  22% /
/dev/sda8             487M   11M  451M   3% /data1
..... (중략) .....
/dev/mapper/vg1-lv1   1.5G   35M  1.4G   3% /lv1
/dev/mapper/vg1-lv2   516M   17M  474M   4% /lv2
```

■ 물리적인 파티션 작업과 LVM 작업 비교

```bash
[이전 작업 방식]

(ㄱ) 파티션 작업
# fdisk /dev/sdc

(ㄴ) 파일시스템 작업
# mkfs.ext4 /dev/sdc1

(ㄷ) 마운트 작업
# vi /etc/fstab
# mkdir /oracle
# mount /oracle

```

```bash
[LVM 작업 방식]

(ㄱ) 파티션 작업(Partition ID (83 -> 8e))
# fdisk /dev/sdc
# pvcreate /dev/sdc1
# vgcreate vg1 /dev/sdc1
# lvcreate -l 100%FREE -n lv1 vg1

(ㄴ) 파일시스템 작업
# mkfs.ext4 /dev/vg1/lv1

(ㄷ) 마운트 작업
# vi /etc/fstab
# mkdir /oracle
# mount /oracle
```

---

# RAID

# 학습목표

---

- RAID LEVEL
- RAID 구성

---

하드웨어를 구성하면서 할 수 있는 디스크 장애에 대한 대비 책?

과거의 유물

디스크:

가격 : 고가

용량 : 저용량

생성되는 데이터는 슬슬 커지기 시작한 시점

새로운 디스크 증설 → 기존에 있는 디스크를 어떻게 써먹을 수 있을까?

## RAID 종류 및 설명

**RAID(Redundant Array of Inexpensive Disks, Redundant Array of Independant Disks)?** 

**여러 디스크를 하나의 디스크처럼 사용할 수 있도록 하면서 동시에 신뢰성을 높이고 성능을 향상시킬 수 있는 저장 장치를 말한다.**

RAID의 종류는 크게 하드웨어 RAID와 소프트웨어 RAID가 있다. 하드웨어 RAID는 안정성이 높은 반면 가격이 비싸다는 단점이 있고, 소프트웨어 RAID는 신뢰성이나 속도가 하드웨어 RAID에 비해 떨어질 수 있으나 비용이 저렴하다는 장점이 있다. 여기서는 하드웨어 RAID에 대해서는 다루지 않기로 하고, 소프트웨어 RAID 구성 방법에 대해 살펴보도록 한다.

### [1] RAID 종류

■ Hardware RAID(Firmware에서 지원, 단위: DISK)

EX) Storage, Array, RAID Controller

-> 성능 우수, 유연성 떨어짐

■ **Software RAID**(OS에서 지원, 단위: Partition)

EX) 운영체제 내에서 Software RAID 툴을 통해 작업

-> 성능 떨어짐, 유연성 우수

[참고] 참고 사이트 : [http://www.acnc.com/raid](http://www.acnc.com/raid)

### [2] RAID 구성 방법의 종류

```bash
RAID 0
RAID 1
RAID 0 + 1
RAID 1 + 0
RAID 2
RAID 3
RAID 4
RAID 5
RAID 6
RAID 7
RAID 53
JBOD
```

### [3] 관련된 용어

**DAS**(**D**irect **A**ttached **S**torage), 직접 연결 저장 장치

- Internal DAS
- External DAS

DAS는 컴퓨터나 서버에 직접 연결된 컴퓨터 저장장치로서, 특별한 고려가 되어 있지 않는 한 다른 장치에서 직접 접근이 불가능하다. DAS의 대안으로서 NAS 또는 SAN 등이 있다. 개별 컴퓨터 사용자의 경우에는 자신이 사용하는 컴퓨터 내부의 하드디스크 드라이브가 Internal DAS의 대표적인 형태가 될 것이다. 

그러나 컴퓨터 내부에 더이상 하드디스크를 연결할수 없는 상황(내부 저장장치 슬롯의 한계)이 온다면  External DAS를 구성하여 사용한다. 단점으로는 디스크와 컴퓨터가 1:1 접속방식이기 때문에 컴퓨터 사용자를 제외한 그 누구도 사용할수 없다. 외장하드디스크가 External DAS의 대표적인 형태이다.

그러나 기업용 사용자들을 위한 저장장치는 여러 대의 컴퓨터에서 공유할 수 있어야 하고, 사용자들은 보다 효율적이며 관리하기 쉬운 장치를 선호하는 경향이 있다. (ktword.co.kr 참조)

**NAS**(**N**etwork **A**ttached **S**torage, Network Area Storage)

- NFS/CIFS(SMB) 서비스 제공(**TCP/IP**)

NAS는 네트워크에 접속되도록 특화된 파일서버이다. 이것은 이더넷이나 TCP/IP와 같은 전통적인 LAN 프로토콜을 사용하며, 오직 유닉스의 NFS와 도스/윈도우의 SMB와 같은 파일 입출력 요청만을 처리한다. (ktword.co.kr 참조)

편리함,  확장성 , 관리에 용이성 

**SAN**(**S**torage **A**rea **N**etwork), 스토리지 전용 네트웍

- Server - SAN(SAN Switch) - Storage

SAN은 대규모 네트웍 사용자들을 위하여 서로 다른 종류의 데이터 저장장치를 관련 데이터 서버와 함께 연결하는 특수목적용 고속 네트웍(또는 서브네트웍)이다. 대체로, SAN은 한 기업의 전체 컴퓨팅 자원을 연결해 놓은 네트웍의 일부가 된다. SAN은 대개 IBM S/390 메인프레임과 같은 다른 컴퓨팅 자원에 아주 근접하여 밀집해 있게 되는 것이 보통이지만, 그러나 백업이나 기록의 영구보관 저장을 위해 ATM이나 SONET 등과 같은 광역통신망 기술을 이용하여 원거리에 있는 장소로 확장될 수도 있다.

SAN은 서로 다른 종류의 저장장치들이 네트웍 서버를 통한 모든 사용자들에 의해 공유될 수 있도록 서로 연결되어 있는 시스템이다. SAN은 IBM의 광섬유인 ESCON과 같은 기존의 통신기술을 이용하거나, 새로운 파이버 채널 기술을 이용할 수도 있다. SAN 시스템 통합 기술을 가진 일부 회사들은, 이것을 하드디스크나 CD-ROM 플레이어와 같이 서로 다른 종류의 저장 장치들에 의해 공유될 수 있는 PC의 공통 저장 버스에 비유하기도 한다.

SAN은 디스크 미러링, 백업 및 복원, 영구보관 및 영구보관용 데이터의 검색, 한 저장장치에서 다른 저장장치로 데이터 이동, 그리고 네트웍 상의 서로 다른 서버들 간에 데이터의 공유 등을 지원한다. (ktword.co.kr 참조)

### RAID 레벨별 차이점

### [1] **RAID 0**

RAID 0은 일반적으로 2개 이상의 하드를 병렬로 연결해서 데이터를 블록(Block) 단위로 분산해서 읽고 쓰는 방식으로 구성된다. 하나의 데이터를 2개 이상의 하드를 이용해 분산시키기 때문에 하드 1개에 데이터를 저장하고 불러오는 것보다 훨씬 빠른 속도를 가질 수 있게 되며 하드 용량도 모두 쓸 수 있다.

이론상으로는 하드 디스크를 추가할 때마다 계속 성능이 향상되기 때문에 여러 대의 하드를 RAID 0으로 묶으면 하드 자체가 가진 물리적인 성능을 넘어 컨트롤러의 대역폭을 충분히 활용할 수 있는 강력한 드라이브를 구축할 수 있다. 하지만 안정성을 체크하는 기능이 없어 만약 1개의 하드라도 완전히 고장 날 경우 RAID 0으로 구성된 모든 데이터를 전부 날리게 된다.

따라서 서버나 워크 스테이션처럼 RAID가 주로 사용되는 분야에서는 절대 RAID 0만으로는 RAID 시스템을 구축하지 않으며, 미러링이나 패리티 정보 저장 등 다른 방식과 함께 구성해 성능/안정성을 동시에 추구하도록 보완된 RAID 규격을 쓴다.

PC에서도 보존해야 할 중요한 데이터가 있는 경우 RAID 0 하드에 저장하지 않는다는 것이 상식이지만, 데이터 보존용이 아니라 프로그램 설치/실행 및 캐시 용도로 사용한다면 충분히 좋은 성과를 누릴 수 있다.

- 최소 드라이브 개수 : 2
- 최대 용량 : 디스크의 수 x 디스크의 용량
- 특징 : 빠른 입출력 속도가 요구되나 장애 복구 능력은 필요 없는 경우에 적합하다.

![1](https://user-images.githubusercontent.com/56914461/103169040-d6fcc880-487b-11eb-885c-0fe16db0c966.png)

참조: [http://www.acnc.com/raid](http://www.acnc.com/raid)

```bash
■ RAID 0 Stripe
---------------------------------------
디스크 사용 효율 ↑
성능(r/w)r(↑), w(↑)
안정성↓
---------------------------------------
```

### [2] RAID 1

RAID 1은 디스크 미러링이라고도 하는데, 데이터의 안정성을 높이기 위해 동일한 데이터를 가진 적어도 두 개의 드라이브로 구성된다.

RAID 1로 구성한 하드는 한 쪽이 망가져도 동일한 데이터가 저장된 다른 하드가 살아있다면 데이터를 복구할 수가 있으며, RAID 구성이 풀리더라도 같은 데이터를 가진 2개의 하드 디스크가 존재하게 된다. 따라서 RAID 1은 데이터 안정성은 높지만 2개의 하드를 RAID 1로 묶으면 같은 데이터를 2개에 기록해 전체 하드 디스크 용량의 절반만 사용하게 된다. 그에 따라 각 드라이브를 동시에 읽을 수 있으므로 읽기 성능은 향상된다. 쓰기 성능은 단일 디스크 드라이브의 경우와 정확히 같다.

RAID-1은 다중 사용자 시스템에서 최고의 성능과 최고의 고장대비 능력을 발휘한다. 모든 레벨에서 가장 높은 비용이 발생하기 때문에 RAID 1은 2개의 하드 디스크를 1조로 묶는 것을 기본으로 하므로 RAID 1로만 시스템을 구현하지 않고 다른 RAID 방식과 합쳐서 구성하게 된다.

- 최소 드라이브 개수 : 2
- 최대 용량 : (디스크의 수/2) x 디스크의 용량
- 특징 : 빠른 기록 속도와 함께 장애 복구 능력이 요구되는 경우에 사용된다. 2대의 드라이브만으로 구성할 수 있기 때문에 작은 시스템에 적합하다.

![2](https://user-images.githubusercontent.com/56914461/103169047-e5e37b00-487b-11eb-8b9f-a1b697054408.png)

```bash
■ RAID 1 Mirroring
---------------------------------------
디스크 사용 효율 ↓↓
성능(r/w)r(↑), w(-)
안정성↑↑
---------------------------------------
```

### [3] RAID 0 + 1

RAID 0+1과 RAID 10(1+0)은 RAID 0(스트라이핑)과 RAID 1(미러링) 방식을 혼합해 만들어졌다는 점에서는 매우 비슷하게 보여진다. 특히 서버/워크스테이션이 아닌 일반 사용자의 PC로 RAID를 구성한다면 많아야 4개 정도까지 묶기 때문에 RAID 0+1과 RAID 1+0을 헷갈릴 수도 있으며 실제 RAID 0+1을 RAID 10으로 표기하는 곳도 많이 있다.

RAID 0+1은 이름 그대로 RAID 0과 RAID 1을 합쳐놓은 방식이다. RAID 0으로 묶은 하드 디스크를 최종적으로 RAID 1로 구성하기 때문에 일반 RAID 1 구성보다 높은 성능을 낼 수 있으며 한쪽 RAID 0에 들어가는 하드들이 모두 고장난 경우에도 나머지 RAID 0 하드를 통해 정상 동작 및 데이터 복구를 할 수 있다.

그래도 RAID 1이 들어가므로 전체 하드 디스크 용량의 절반 밖에 사용하지 못한다는 단점은 같지만, 성능과 안정성을 동시에 추구할 수 있는데다 따로 패리티 정보를 사용하지 않기 때문에 패리티 정보를 별도로 처리할 필요가 없다. 또한 RAID 기능을 지원하는 웬만한 메인보드 컨트롤러 및 보급형 RAID 카드에서 RAID 0+1을 기본 지원하고 있어 RAID 3, 4와 달리 고급형 컨트롤러가 필요하지 않다.

- 최소 드라이브 개수 : 4
- 최대 용량 : (디스크의 수/2) x 디스크의 용량
- 특징 : 일반 RAID 1 구성보다 높은 성능을 낼 수 있으며 한쪽 RAID 0에 들어가는 하드들이 모두 고장난 경우에도 나머지 RAID 0 하드를 통해 정상 동작 및 데이터 복구를 할 수 있다.

![3](https://user-images.githubusercontent.com/56914461/103169051-eed44c80-487b-11eb-85b4-20984d47ce96.png)


### [4] RAID 1 + 0 (RAID 10)

RAID 10(1+0)은 RAID 1로 구성된 하드들을 최종적으로 RAID 0 방식으로 병렬구성(striping)해서 성능을 높이게 된다. 역시 RAID 1의 미러링을 기본으로 하고 있으므로 하드 1개가 고장나도 그와 함께 미러링 된 하드를 통해 데이터 복구가 가능하다.

![4](https://user-images.githubusercontent.com/56914461/103169060-027fb300-487c-11eb-9dff-3b0b20fc93fb.png)

RAID 0+1과 RAID 10은 4개의 하드 디스크로 RAID를 구성할 경우 용량이나 구조에서 별 차이가 없어 보이지만 6개 이상의 하드를 사용하게 되면 위와 같은 차이가 생긴다. RAID 0+1은 RAID 0으로 구성된 하드들을 최종적으로 RAID 1로 묶는 것이라 각각 3개씩 하드가 나눠지며, RAID 10은 2개씩 RAID 1으로 묶여있는 하드들이 RAID 0으로 구성된다.

RAID 0+1은 양쪽 RAID 0 구성 중 하나씩 고장이 나면 전체 데이터가 손실되고, RAID 10은 미러링 된 하드 2개가 동시에 고장 날 경우 전체 데이터가 날아간다. 물론 그 대신 RAID 0+1은 미러링 된 한쪽이 하드 전체가 망가져도 데이터를 살릴 수 있고, RAID 10도 각 RAID 1 묶음에서 하나씩이 고장이 나도 이상 없이 동작한다.

그러나 RAID 0+1의 경우 1개의 하드만 고장 나서 복구해도 다른 RAID 0 구성에서 나머지 하드까지 데이터 전체(A1~3, B1~3, C1~3)를 복구해야 하지만, RAID 10으로 만든 시스템은 고장난 하드가 A2, B2, C3 가 저장된 하드 1개라고 하면 미러링으로 묶인 하드를 통해 A2, B2, C2 데이터만 복구하면 되므로 실제로 운영하는데는 RAID 10이 훨씬 유리하다.

### [5] RAID 2

RAID 2는 RAID 0처럼 스트라이핑 방식이지만 에러 체크와 수정을 할 수 있도록 Hamming Code를 사용하고 있는 것이 특징이다. 하드 디스크에서 ECC 기능을 지원하지 않기 때문에 ECC(Error Correction Code)을 별도의 드라이브에 저장하는 방식으로 처리된다.

하지만 ECC를 위한 드라이브가 손상될 경우는 문제가 발생할 수 있으며, 패리티 정보를 하나의 하드 드라이브에 저장하는 RAID 4가 나오면서 거의 사용되지 않는 방식이라고 하겠다. 게다가 모든 SCSI 드라이브는 에러검출능력을 갖고 있기 때문에 SCSI 드라이브를 사용할 경우 이 레벨은 별로 쓰이지 않는다.

- 모든 현행 드라이브들이 ECC를 탑재하고 있기 때문에 거의 사용되지 않는다.

![5](https://user-images.githubusercontent.com/56914461/103169062-0a3f5780-487c-11eb-9a7e-fd101b9aa004.png)

### [6] RAID 3

RAID 3, RAID 4는 RAID 0, 1의 문제점을 보완하기 위한 방식으로 3, 4로 나뉘긴 하지만 RAID 구성 방식은 거의 같다. RAID 3/4는 기본적으로 RAID 0과 같은 스트라이핑(Striping) 구성을 하고 있어 성능을 보완하고 디스크 용량을 온전히 사용할 수 있게 해주는데, 여기에 추가로 에러 체크 및 수정을 위해서 패리티(Parity) 정보를 별도의 디스크에 따로 저장하게 된다.

RAID 3는 데이터를 바이트(byte) 단위로 나눠 디스크에 동등하게 분산 기록하며, RAID 4는 데이터를 블럭 단위로 나눠 기록하므로 완벽하게 동일하진 않다. RAID 3는 드라이브 동기화가 필수적이라 많이 사용되지 않고 RAID 4를 더 많이 쓴다고 한다.

RAID 0+1과는 다르게 패리티 정보 기록용 하드 1개만을 제외한 나머지 하드를 모두 스트라이핑으로 묶어 사용하게 되므로 저장 용량에서도 유리하며, 패리티 정보를 별도의 디스크에 저장하므로 스트라이핑 하드 가운데 1개가 고장나도 패리티 정보 디스크를 통해 복구가 가능하다. 하지만 패리티 디스크 쪽도 데이터를 갱신해줘야 하므로 느려질 수 있으며 패리티 디스크가 고장나면 문제가 생길 수 있다. 스트라이핑과 달리 패리티 정보 저장 및 추후 복구를 위해 XOR 연산이 필요하며, 이 때문에 고가의 RAID 컨트롤러는 XOR 연산을 담당하는 프로세서를 따로 장착하고 있다.

- 최소 드라이브 개수 : 3
- 최대 용량 : (디스크의 수 - 1) x 각 디스크의 용량
- 특징 : level 4와 유사하나 효율적인 동작을 위해 동기가능한(synchronized-spindle) 드라이브를 필요로 한다.

![6](https://user-images.githubusercontent.com/56914461/103169065-10cdcf00-487c-11eb-9e45-e5662fbd5ad6.png)

### [7] RAID 4

RAID4은 대형 스트라이프를 사용하며, 이는 사용자가 어떤 단일 드라이브로부터라도 레코드를 읽을 수 있다는 것을 의미한다. 이것은 데이터를 읽을 때 중첩 입출력의 장점을 취할 수 있도록 한다. 모든 쓰기 작업은 패리티 드라이브를 갱신해야하므로, 입출력의 중첩은 불가능하다. RAID-4는 RAID-5에 비해 장점이 없다.

- 최소 드라이브 개수 : 3
- 최대 용량 : (디스크의 수 - 1) x 디스크의 용량
- 특징 : 저렴한 가격으로 장애 복구 능력이 요구되거나 빠른 판독 속도가 필요한 경우에 사용된다. 다량의 데이터 전송이 요하는 CAD나 이미지 작업에 적합하다.

![7](https://user-images.githubusercontent.com/56914461/103169068-16c3b000-487c-11eb-81e9-9bbf93133e0d.png)

### [8] RAID 5

RAID 5는 RAID 3, 4에서 별도의 패리티 정보 디스크를 사용함으로써 발생하는 문제점을 보완하는 방식으로 패리티 정보를 스트라이핑으로 구성된 디스크 내에서 처리하게 만들었다. 물론 패리티 정보는 데이터가 저장된 디스크와는 물리적으로 다른 디스크에 저장되도록 만들어 1개의 하드가 고장 나더라도 남은 하드들을 통해 데이터를 복구할 수 있도록 했다. 최소 3개부터 구성이 가능하며 하드 1개 용량만 빠지게 되므로 미러링으로 처리되는 RAID 1보다 저장 공간도 크다.

패리티 정보가 데이터가 저장된 디스크와는 다른 디스크에 저장되기 때문에 위의 RAID 5 구성에서 만약에 3번 하드(Disk3)가 고장날 경우 A3, C2, D2, 데이터는 다른 하드에 별도로 저장된 패리티 정보를 통해서 복구하고 B 패리티 정보는 나머지 하드에 있는 B1, B2, B3 데이터를 토대로 다시 작성할 수 있다. 그러나 별도로 패리티 정보를 저장하는 작업을 해야 하므로 RAID 1보다 쓰기 성능이 떨어진다.

RAID 3, 4와 달리 패리티 정보가 저장된 디스크가 따로 없어 패리티 디스크 고장과 같은 문제에서 자유롭고 실제 서버/워크스테이션에서 가장 많이 사용되는 방식이기도 하다. 보드나라 서버 역시 3개의 하드로 RAID 5를 구성하고 있다.

RAID 5 역시 XOR 연산을 필요로 하기 때문에 PCI 방식의 보급형 RAID 컨트롤러 카드나 데스크탑 메인보드의 내장 RAID 컨트롤러에서는 그 동안 거의 지원하지 않았던 규격이기도 하지만, 컴퓨터 성능이 올라간 요즘에는 메인보드 사우스브릿지 칩셋에 RAID 5 기능을 넣는 추세로 가고 있다.

- 최소 드라이브 개수 : 3
- 최대 용량 : (디스크의 수 - 1) x 디스크의 용량
- 특징 : 작고 랜덤한 입출력이 많은 경우 더 나은 성능을 제공한다. 빠른 기록속도가 필수적이지 않다면, 일반적인 다중사용자 환경을 위해 가장 좋은 선택이다. 그러나 최소한 3대, 일반적으로는 5대 이상의 드라이브가 필요하다.

![8](https://user-images.githubusercontent.com/56914461/103169070-1d522780-487c-11eb-8c3c-8c4f5d960e17.png)

```bash
■ RAID 5 Stripe with parity
---------------------------------------
디스크 사용 효율 ↓
성능(r/w)r(↑), w(↓↓)
안정성↑
---------------------------------------
```

### [9] RAID 6

RAID 6는 RAID 5와 같은 개념이지만 다른 드라이브들 간에 분포되어 있는 2차 패리티 정보를 넣어 2개의 하드에 문제가 생겨도 데이터를 복구할 수 있게 고안되었다. 이론상 하드 3개부터 만들 수 있지만 이럴 경우는 1개의 하드에 저장될 용량을 2개의 하드로 패리티 정보를 저장하는 꼴이므로 전체 용량의 1/3만 사용 가능하게 된다. 따라서 보통 4개 이상의 하드 디스크로 구성하며 RAID 5보다 더욱 데이터 안정성을 고려하는 시스템에서 사용한다.

- 최소 드라이브 개수 : 3
- 최대 용량 : (디스크의 수 - 2) x 디스크의 용량

    ![9](https://user-images.githubusercontent.com/56914461/103169077-23e09f00-487c-11eb-9005-c9c178891517.png)


### [10] RAID 7

RAID7은 컨트롤러로서 내장되어 있는 실시간 운영체계를 사용하며, 속도가 빠른 버스를 통한 캐시, 독자적인 컴퓨터의 여러 가지 특성들을 포함하고 있다. 현재 단 하나의 업체만이 이 시스템을 제공한다. 다시말해 패리티 정보를 처리하기 위해 RAID5, 6번은 속도가 저하 되었었다. 그러나 RAID7번의 경우 패리티 정보를 처리하는 하나의 CPU가 존재해 시스템의 성능을 높여준다.

![10](https://user-images.githubusercontent.com/56914461/103169082-2b07ad00-487c-11eb-8e50-b27df67ce1c2.png)

### [11] RAID 53

RAID-53은 각 스트라이프는 RAID-3 디스크 에레이인 스트라이프 어레이를 제공한다. 이 방식은 RAID-3보다 높은 성능을 제공하지만, 값이 더 비싸다.

![11](https://user-images.githubusercontent.com/56914461/103169084-31962480-487c-11eb-8d7c-62ab8ea08405.png)

### [12] JBOD

JBOD는 Just Bunch of Disks의 약어로 사실은 스트라이핑이나 미러링 방식을 쓰지 않았기 때문에 RAID 규격이라고 할 수 없다. 다만 여러 대의 하드 디스크를 묶어 하나의 하드처럼 보이게 만든다는 점에서 RAID와 비슷하기 때문에 함께 쓰이고, RAID 지원 컨트롤러에서 JBOD 역시 지원하는 일이 많은 것이다.

JBOD는 단순히 물리적인 여러 하드를 모아서 하나의 큰 용량을 제공하는 하드처럼 보이게 만드는 방식으로 스패닝(Spanning)으로도 불린다. 디스크 전체 용량을 사용한다는 점에서 RAID 0과 같지만 RAID 0처럼 데이터를 분산 저장하는게 아니라 디스크 순서대로 저장하기 때문에 대용량 디스크를 만들 수 있지만 실제 성능 향상은 없다. 그러나 디스크 종류/용량에 제한을 받게 되는 RAID 규격과 달리 각기 다른 용량의 디스크도 묶을 수 있다는 장점이 있다.

![12](https://user-images.githubusercontent.com/56914461/103169089-378c0580-487c-11eb-8917-862de796111b.png)

**과정 진행에서는 실습은 진행하지 않습니다.**

## RAID 실습

### [1] mdadm 명령어

NAME

mdadm - manage MD devices aka Linux Software RAID

SYNOPSIS

mdadm [mode] <raiddevice> [options] <component-devices>

DESCRIPTION

RAID devices are virtual devices created from two or more real block devices. This

allows multiple devices (typically disk drives or partitions thereof) to be com-

bined into a single device to hold (for example) a single filesystem. Some RAID

levels include redundancy and so can survive some degree of device failure.

Linux Software RAID devices are implemented through the md (Multiple Devices)

device driver.

Currently, Linux supports LINEAR md devices, RAID0 (striping), RAID1 (mirroring),

RAID4, RAID5, RAID6, RAID10, MULTIPATH, and FAULTY.

MULTIPATH is not a Software RAID mechanism, but does involve multiple devices: each

device is a path to one common physical storage device.

FAULTY is also not true RAID, and it only involves one device. It provides a layer

over a true device that can be used to inject faults.

OPTIONS

Options for selecting a mode are:

-C, --create

Create a new array.

Options that are not mode-specific are:

-V, --version

Print version information for mdadm.

-s, --scan

Scan config file or /proc/mdstat for missing information. In general, this

option gives mdadm permission to get any missing information (like component

devices, array devices, array identities, and alert destination) from the

configuration file (see previous option); one exception is MISC mode when

using --detail or --stop, in which case --scan says to get a list of array

devices from /proc/mdstat.

-f, --force

Be more forceful about certain operations. See the various modes for the

exact meaning of this option in different contexts.

For create, build, or grow:

-n, --raid-devices=

Specify the number of active devices in the array. This, plus the number of

spare devices (see below) must equal the number of component-devices

(including "missing" devices) that are listed on the command line for --cre-

ate. Setting a value of 1 is probably a mistake and so requires that

--force be specified first. A value of 1 will then be allowed for linear,

multipath, raid0 and raid1. It is never allowed for raid4 or raid5.

This number can only be changed using --grow for RAID1, RAID5 and RAID6

arrays, and only on kernels which provide necessary support.

-l, --level=

Set raid level. When used with --create, options are: linear, raid0, 0,

stripe, raid1, 1, mirror, raid4, 4, raid5, 5, raid6, 6, raid10, 10, multi-

path, mp, faulty. Obviously some of these are synonymous.

When used with --build, only linear, stripe, raid0, 0, raid1, multipath, mp,

and faulty are valid.

For Manage mode:

-r, --remove

remove listed devices. They must not be active. i.e. they should be failed

or spare devices. As well as the name of a device file (e.g. /dev/sda1)

the words failed and detached can be given to --remove. The first causes

all failed device to be removed. The second causes any device which is no

longer connected to the system (i.e an ’open’ returns ENXIO) to be removed.

This will only succeed for devices that are spares or have already been

marked as failed.

-f, --fail

mark listed devices as faulty. As well as the name of a device file, the

word detached can be given. This will cause any device that has been

detached from the system to be marked as failed. It can then be removed.

For Misc mode:

-D, --detail

Print detail of one or more md devices.

-S, --stop

deactivate array, releasing all resources.

--zero-superblock

If the device contains a valid md superblock, the block is overwritten with

zeros. With --force the block where the superblock would be is overwritten

even if it doesn’t appear to be valid.

[명령어 형식]

```bash
■ RAID 설정
# mdadm --create /dev/md0 --level=<RAID 레벨> --raid-device=<RAID 구성할 Disk수> <디스크장치명> ...
# mdadm --create /dev/md0 --level=1 --raid-device=2 /dev/sdc1 /dev/sdd1
# mdadm -C /dev/md0 -l 1 -n 2 /dev/sdc1 /dev/sdd1
/* --create :   -C   */
/* --level=1 :   -l 1 */
/* --raid-device=2 :   -n 2 */

■ RAID 장치 설정 확인
# mdadm --detail /dev/md0                     /* --detail  :  -D */

■ /etc/mdadm.conf 파일 설정
# echo "DEVIECE partitions" > /etc/mdadm.conf 
# mdadm --detail --scan >> /etc/mdadm.conf    /* --scan    : -s  */
# cat /etc/mdadm.conf
DEVIECE partitions
ARRAY /dev/md0 level=raid0 num-devices=2 metadata=0.90 UUID=26989dc0:d41ac869:f76fb5d7:a077d304
■ RAID 삭제
(ㄱ) RAID Device stop
# mdadm --stop /dev/md0    /* --stop   : -S */

(ㄴ) RAID Device remove
# mdadm --remove /dev/md0  /* --remove : -r */

(ㄷ) superblock 정보 삭제
# mdadm --zero-superblock /dev/sdb1 /dev/sdc1 /dev/sdd1
```

### [2] mdadm 명령어 실습

### [EX1] RAID 구성을 위한 디스크 설정(System Partition ID 설정: fd)

```bash
① mdadm 패키지 설치 확인
# rpm -qa | grep mdadm   (# yum -y install mdadm)
mdadm-2.6.9-2.el5
# mdadm -V
mdadm - v2.6.9 - 10th March 2009
② 보유한 디스크 확인
# ls -l /dev/sd?
brw-r----- 1 root disk 8,   0 Apr 29 14:50 /dev/sda   <---- OS Disk
brw-r----- 1 root disk 8,  16 Apr 29 14:50 /dev/sdb   <---- /tsetmount
brw-r----- 1 root disk 8,  32 Apr 29 14:50 /dev/sdc
brw-r----- 1 root disk 8,  48 Apr 29 14:50 /dev/sdd
brw-r----- 1 root disk 8,  64 Apr 29 14:50 /dev/sde
brw-r----- 1 root disk 8,  80 Apr 29 14:50 /dev/sdf
brw-r----- 1 root disk 8,  96 Apr 29 15:08 /dev/sdg
brw-r----- 1 root disk 8, 112 Apr 29 15:29 /dev/sdh
③ 디스크(EX: /dev/sdc, /dev/sdd) 파티션 타입(파티션 ID) 변경 및 확인
# fdisk /dev/sdc
Command (m for help): p

Disk /dev/sdc: 1073 MB, 1073741824 bytes
255 heads, 63 sectors/track, 130 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes

   Device Boot      Start         End      Blocks   Id  System
/dev/sdc1               1         130     1044193+  8e  Linux LVM

Command (m for help): t
Selected partition 1
Hex code (type L to list codes): L

 0  Empty           1e  Hidden W95 FAT1 80  Old Minix       bf  Solaris        
 1  FAT12           24  NEC DOS         81  Minix / old Lin c1  DRDOS/sec (FAT-
 2  XENIX root      39  Plan 9          82  Linux swap / So c4  DRDOS/sec (FAT-
 3  XENIX usr       3c  PartitionMagic  83  Linux           c6  DRDOS/sec (FAT-
 4  FAT16 <32M      40  Venix 80286     84  OS/2 hidden C:  c7  Syrinx         
 5  Extended        41  PPC PReP Boot   85  Linux extended  da  Non-FS data    
 6  FAT16           42  SFS             86  NTFS volume set db  CP/M / CTOS / .
 7  HPFS/NTFS       4d  QNX4.x          87  NTFS volume set de  Dell Utility   
 8  AIX             4e  QNX4.x 2nd part 88  Linux plaintext df  BootIt         
 9  AIX bootable    4f  QNX4.x 3rd part 8e  Linux LVM       e1  DOS access     
 a  OS/2 Boot Manag 50  OnTrack DM      93  Amoeba          e3  DOS R/O        
 b  W95 FAT32       51  OnTrack DM6 Aux 94  Amoeba BBT      e4  SpeedStor      
 c  W95 FAT32 (LBA) 52  CP/M            9f  BSD/OS          eb  BeOS fs        
 e  W95 FAT16 (LBA) 53  OnTrack DM6 Aux a0  IBM Thinkpad hi ee  EFI GPT        
 f  W95 Ext'd (LBA) 54  OnTrackDM6      a5  FreeBSD         ef  EFI (FAT-12/16/
10  OPUS            55  EZ-Drive        a6  OpenBSD         f0  Linux/PA-RISC b
11  Hidden FAT12    56  Golden Bow      a7  NeXTSTEP        f1  SpeedStor      
12  Compaq diagnost 5c  Priam Edisk     a8  Darwin UFS      f4  SpeedStor      
14  Hidden FAT16 <3 61  SpeedStor       a9  NetBSD          f2  DOS secondary  
16  Hidden FAT16    63  GNU HURD or Sys ab  Darwin boot     fb  VMware VMFS    
17  Hidden HPFS/NTF 64  Novell Netware  b7  BSDI fs         fc  VMware VMKCORE 
18  AST SmartSleep  65  Novell Netware  b8  BSDI swap       fd  Linux raid auto
1b  Hidden W95 FAT3 70  DiskSecure Mult bb  Boot Wizard hid fe  LANstep        
1c  Hidden W95 FAT3 75  PC/IX           be  Solaris boot    ff  BBT            
Hex code (type L to list codes): fd
Changed system type of partition 1 to fd (Linux raid autodetect)

Command (m for help): p

Disk /dev/sdc: 1073 MB, 1073741824 bytes
255 heads, 63 sectors/track, 130 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes

   Device Boot      Start         End      Blocks   Id  System
/dev/sdc1               1         130     1044193+  fd  Linux raid autodetect

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
# fdisk /dev/sdd
Command (m for help): p

Disk /dev/sdd: 1073 MB, 1073741824 bytes
255 heads, 63 sectors/track, 130 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes

   Device Boot      Start         End      Blocks   Id  System
/dev/sdd1               1         130     1044193+  8e  Linux LVM

Command (m for help): t
Selected partition 1
Hex code (type L to list codes): fd
Changed system type of partition 1 to fd (Linux raid autodetect)

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.

# fdisk /dev/sde
Command (m for help): p

Disk /dev/sde: 1073 MB, 1073741824 bytes
255 heads, 63 sectors/track, 130 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes

   Device Boot      Start         End      Blocks   Id  System
/dev/sde1               1         130     1044193+  8e  Linux LVM

Command (m for help): t
Selected partition 1
Hex code (type L to list codes): fd
Changed system type of partition 1 to fd (Linux raid autodetect)

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
# fdisk -l | grep raid
/dev/sdc1               1         130     1044193+  fd  Linux raid autodetect
/dev/sdd1               1         130     1044193+  fd  Linux raid autodetect
/dev/sde1               1         130     1044193+  fd  Linux raid autodetect
```

### [EX2] RAID 0 구성 실습

```bash
■ RAID 0 구성 절차
(ㄱ) 파티션 타입 설정
# fdisk /dev/sdc 
# fdisk /dev/sdd 
(ㄴ) RAID 구성
# mdadm --create /dev/md0 --level=0 --raid-device=2 /dev/sdc1 /dev/sdd1
# echo "DEVICE partitions" > /etc/mdadm.conf 
# mdadm --detail --scan >> /etc/mdadm.conf 
(ㄷ) F/S 생성
# mkfs.ext4 /dev/md0 
(ㄹ) 마운트
# vi /etc/fstab 
# mkdir /raid0 ; mount /raid0 

■ RAID 0 구성 해제 절차
(ㄱ) 언마운트
# vi /etc/fstab 
# umount /raid0 
(ㄴ) RAID 구성 삭제
# mdadm --stop /dev/md0 
# mdadm --remove /dev/md0 
# mdadm --zero-superblock /dev/sdc1 /dev/sdd1 
# rm /etc/mdadm.conf
```

```bash
① mdadm 명령어를 사용하여 RAID 0 볼륨 구성 및 확인
# mdadm --create /dev/md0 --level=0 --raid-device=2 /dev/sdc1 /dev/sdd1
mdadm: array /dev/md0 started.
# cat /proc/mdstat
Personalities : [raid0]
md0 : active raid0 sdd1[1] sdc1[0]
      2088192 blocks 64k chunks

unused devices: <none>
# mdadm --detail /dev/md0
/dev/md0:
        Version : 0.90
  Creation Time : Sun Apr 29 17:23:02 2012
     Raid Level : raid0
     Array Size : 2088192 (2039.59 MiB 2138.31 MB)
   Raid Devices : 2
  Total Devices : 2
Preferred Minor : 0
    Persistence : Superblock is persistent

    Update Time : Sun Apr 29 17:23:02 2012
          State : clean
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 64K

           UUID : ed833cba:3f84d5a4:c0582354:d39542d3
         Events : 0.1

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       49        1      active sync   /dev/sdd1
② /etc/mdadm.conf 파일 생성

By default, changes made by the mdadm command only apply to the curent session, and will not survive a system restart. At boot time, the mdmonitor service reads the content of the /etc/mdadm.conf configuration file to see which RAID devices to start. 

(현재) mdadm CMD 
(부팅) /etc/mdadm.conf 

# ls -l /etc/mdadm.conf
ls: /etc/mdadm.conf: No such file or directory
# mdadm --detail --scan                      (# mdadm --detail --brief /dev/md0)
ARRAY /dev/md0 level=raid0 num-devices=2 metadata=0.90 UUID=ed833cba:3f84d5a4:c0582354:d39542d3
# echo "DEVICE partitions" > /etc/mdadm.conf 
# mdadm --detail --scan >> /etc/mdadm.conf   (# mdadm --detail --brief /dev/md0 >> /etc/mdadm.conf)
# cat /etc/mdadm.conf
DEVICE partitions
ARRAY /dev/md0 level=raid0 num-devices=2 metadata=0.90 UUID=ed833cba:3f84d5a4:c0582354:d39542d3
③ 파일시스템 생성
# mkfs.ext4 /dev/md0
mke2fs 1.39 (29-May-2006)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
131072 inodes, 262080 blocks
13104 blocks (5.00%) reserved for the super user
..... (중략) .....
Writing superblocks and filesystem accounting information: done

This filesystem will be automatically checked every 33 mounts or
180 days, whichever comes first.  Use tune2fs -c or -i to override.
# tune2fs -l /dev/md0       /* 파일시스템 superblock 정보 확인 */
tune2fs 1.39 (29-May-2006)
Filesystem volume name:   <none>
Last mounted on:          <not available>
Filesystem UUID:          ccf90a90-ff3d-44e1-931c-42da04241424
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal resize_inode dir_index filetype sparse_super large_file
Default mount options:    (none)
Filesystem state:         clean
Errors behavior:          Continue
..... (중략) .....
④ 장치 마운트 및 확인
# mkdir -p /raid0 
# mount /dev/md0 /raid0 
# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1              17G  3.4G   12G  22% /
/dev/sda8             487M   11M  451M   3% /data1
/dev/sda7             487M   11M  451M   3% /data2
/dev/sda6             487M   11M  451M   3% /data3
/dev/sda5             487M   11M  451M   3% /data4
/dev/sda3             487M   11M  451M   3% /home
tmpfs                 506M     0  506M   0% /dev/shm
/dev/md0              2.0G   36M  1.9G   2% /raid0
# vi /etc/fstab
----------------------------
....
#
# RAID Configuration
#
/dev/md0               /raid0                  ext4    defaults        0 0 
---------------------------
⑤ 언마운트 및 확인
# umount /raid0 
# vi /etc/fstab
---------------------------
....
#
# RAID Configuration
#
#/dev/md0               /raid0                  ext4    defaults        0 0 
---------------------------

-> 실습에서는 정의하고 hash 처리를 한다.

장치해제 
# umount /raid0 

⑥ RAID 설정 삭제
# mdadm --stop /dev/md0
mdadm: stopped /dev/md0
# mdadm --remove /dev/md0 [이전 버전에서는 따로 삭제를 해야하였음]
# mdadm --zero-superblock /dev/sdc1 /dev/sdd1 

# cat /proc/mdstat
Personalities : [raid0]
unused devices: <none>
# rm –f /etc/mdadm.conf 
#
```

■ 일반적인 디스크 구성 & LVM & RAID 비교

```bash
# fdisk /dev/sdc(83)
# fdisk /dev/sdd(83)

# mkfs.ext4 /dev/sdc1
# mkfs.ext4 /dev/sdd1
# vi /etc/fstab
# mkdir /oracle /data
# mount /oracle
# mount /data
```

```bash
# fdisk /dev/sdc(8e)
# fdisk /dev/sdd(8e)
# pvcreate /dev/sd[cd]1
# vgcreate vg1 /dev/sd[cd]1
# lvcreate -L 1G -n lv1 vg1
# lvcreate –l 100%FREE \
-n lv2 vg1
# mkfs.ext4 /dev/vg1/lv1
# mkfs.ext4 /dev/vg1/lv2
# vi /etc/fstab
# mkdir /oracle /data
# mount /oracle
# mount /data
```

```bash
# fdisk /dev/sdc(fd)
# fdisk /dev/sdd(fd)
# mdadm --create /dev/md0 \
--level=0 —raid-device=2 \
/dev/sdc1 /dev/sdd1

# mkfs.ext4 /dev/md0
# vi /etc/fstab
# mkdir /oracle
# mount /oracle
```

### [EX3] RAID 1 구성

```bash
■ RAID 1 구성 절차
(ㄱ) 파티션 타입 설정
# fdisk /dev/sdc 
# fdisk /dev/sdd 
(ㄴ) RAID 구성
# mdadm --create /dev/md0 --level=1 --raid-device=2 /dev/sdc1 /dev/sdd1
# echo "DEVICE partitions" > /etc/mdadm.conf 
# mdadm --detail --scan >> /etc/mdadm.conf 
(ㄷ) F/S 생성
# mkfs.ext4 /dev/md0 
(ㄹ) 마운트
# vi /etc/fstab 
# mkdir /raid1 ; mount /raid1 

■ RAID 1 구성 해제 절차
(ㄱ) 언마운트
# vi /etc/fstab 
# umount /raid1 
(ㄴ) RAID 구성 삭제
# mdadm --stop /dev/md0 
# mdadm --remove /dev/md0 
# mdadm --zero-superblock /dev/sdc1 /dev/sdd1 
# rm /etc/mdadm.conf

```

① RAID 1 구성 및 확인

[TERM1] 명령어 윈도우

```bash
# mdadm --create /dev/md0 --level=1 --raid-device=2 /dev/sdc1 /dev/sdd1
mdadm: /dev/sdc1 appears to contain an ext2fs file system
    size=2088192K  mtime=Sun Apr 29 17:25:13 2012
mdadm: /dev/sdc1 appears to be part of a raid array:
    level=raid0 devices=2 ctime=Sun Apr 29 17:23:02 2012
mdadm: /dev/sdd1 appears to be part of a raid array:
    level=raid0 devices=2 ctime=Sun Apr 29 17:23:02 2012
Continue creating array? y
mdadm: array /dev/md0 started.
```

[TERM2] 모니터링 윈도우

```bash
# watch cat /proc/mdstat        (# watch CMD)
Every 2.0s: cat /proc/mdstat                                                Wed Jan 22 16:32:41 2014

Personalities : [raid6] [raid5] [raid4] [raid0] [raid1]
md0 : active raid1 sdd1[1] sdc1[0]
      1044096 blocks [2/2] [UU]
      [=======>.............]  resync = 35.3% (370176/1044096) finish=0.3min speed=37017K/sec

unused devices: <none>

Every 2.0s: cat /proc/mdstat                                                Wed Jan 22 16:33:09 2014

Personalities : [raid6] [raid5] [raid4] [raid0] [raid1]
md0 : active raid1 sdd1[1] sdc1[0]
      1044096 blocks [2/2] [UU]

unused devices: <none>
```

[참고] watch & while ~ do ~ done

```bash
# watch CMD
or
# while true
> do
> clear
> CMD
> sleep 2
> done
```

[TERM1] 명령어 윈도우

```bash
# mdadm --detail /dev/md0
/dev/md0:
        Version : 0.90
  Creation Time : Sun Apr 29 17:32:16 2012
     Raid Level : raid1
     Array Size : 1044096 (1019.80 MiB 1069.15 MB)
  Used Dev Size : 1044096 (1019.80 MiB 1069.15 MB)
   Raid Devices : 2
  Total Devices : 2
Preferred Minor : 0
    Persistence : Superblock is persistent

    Update Time : Sun Apr 29 17:32:41 2012
          State : clean
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           UUID : acf2acdd:bbd1bfde:9fedec91:cc714f70
         Events : 0.2

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       49        1      active sync   /dev/sdd1

② /etc/mdadm.conf 파일 생성
# echo "DEVICE partitions" > /etc/mdadm.conf 
# mdadm --detail --scan >> /etc/mdadm.conf 
# cat /etc/mdadm.conf
DEVICE partitions
ARRAY /dev/md0 level=raid1 num-devices=2 metadata=0.90 UUID=acf2acdd:bbd1bfde:9fedec91:cc714f70
③ 파일시스템 생성
# mkfs.ext4 /dev/md0
mke2fs 1.39 (29-May-2006)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
130560 inodes, 261024 blocks
13051 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=268435456
8 block groups
32768 blocks per group, 32768 fragments per group
16320 inodes per group
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376

Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done

This filesystem will be automatically checked every 37 mounts or
180 days, whichever comes first.  Use tune2fs -c or -i to override.
# tune2fs -l /dev/md0  /* superblock information */
-> 출력 정보 생략

# dumpe2fs /dev/md0    /* (superblock + block group) information */
-> 출력 정보 생략
④ 마운트 및 확인
# mkdir -p /raid1 
# mount /dev/md0 /raid1 
# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1              17G  3.4G   12G  22% /
/dev/sda8             487M   11M  451M   3% /data1
/dev/sda7             487M   11M  451M   3% /data2
/dev/sda6             487M   11M  451M   3% /data3
/dev/sda5             487M   11M  451M   3% /data4
/dev/sda3             487M   11M  451M   3% /home
tmpfs                 506M     0  506M   0% /dev/shm
/dev/md0             1004M   18M  936M   2% /raid1

# vi /etc/fstab
..... (중략) .....
#
# (4) RAID Configuration
#
#/dev/md0               /raid0                  ext4    defaults        1 2
/dev/md0                /raid1                  ext4    defaults        1 2
```

```bash
⑤ 파일 생성 테스트
                       +-----> /dev/sdc1
                       |
/dev/md0 -----+
         (/raid1/*)    |
                       +-----> /dev/sdd1
```

```bash
# cd /raid1 
# cp /etc/services file1 
# cp file1 file2 
# cp file1 file3 
# cp file1 file4 
# cp file1 file5 
# ls
file1  file2  file3  file4  file5  lost+found/

⑥ 언마운트 
# cd 
# umount /raid1 
# vi /etc/fstab
	#
	# (4) RAID Configuration
	#
	#/dev/md0               /raid0                  ext4    defaults        1 2
	#/dev/md0               /raid1                  ext4    defaults        1 2

⑦ RAID 설정 해제
# mdadm --stop /dev/md0
mdadm: stopped /dev/md0
# mdadm --remove /dev/md0 
# mdadm --zero-superblock /dev/sdc1 /dev/sdd1 

# cat /proc/mdstat
Personalities : [raid0] [raid1] 
unused devices: <none>
# **rm -f /etc/mdadm.conf** 
#

⑧ 개별적인 마운트
# mount /dev/sdc1 /raid0 
# mount /dev/sdd1 /raid1 
# ls -l /raid0 /raid1
/raid0:
total 1.8M
-rw-r--r-- 1 root root 354K Apr 29 17:38 file1
-rw-r--r-- 1 root root 354K Apr 29 17:38 file2
-rw-r--r-- 1 root root 354K Apr 29 17:38 file3
-rw-r--r-- 1 root root 354K Apr 29 17:38 file4
-rw-r--r-- 1 root root 354K Apr 29 17:39 file5
drwx------ 2 root root  16K Apr 29 17:35 lost+found/

/raid1:
total 1.8M
-rw-r--r-- 1 root root 354K Apr 29 17:38 file1
-rw-r--r-- 1 root root 354K Apr 29 17:38 file2
-rw-r--r-- 1 root root 354K Apr 29 17:38 file3
-rw-r--r-- 1 root root 354K Apr 29 17:38 file4
-rw-r--r-- 1 root root 354K Apr 29 17:39 file5
drwx------ 2 root root  16K Apr 29 17:35 lost+found/

# cd 
# umount /raid0 
# umount /raid1
```

### [EX4] RAID5 구성

```bash
■ RAID 5 구성 절차
(ㄱ) 파티션 타입 설정
# fdisk /dev/sdc 
# fdisk /dev/sdd 
# fdisk /dev/sde 
(ㄴ) RAID 구성
# mdadm --create /dev/md0 --level=5 --raid-device=3 /dev/sdc1 /dev/sdd1 /dev/sde1 
# echo "DEVICE partitions" > /etc/mdadm.conf 
# mdadm --detail --scan >> /etc/mdadm.conf 
(ㄷ) F/S 생성
# mkfs.ext4 /dev/md0 
(ㄹ) 마운트
# vi /etc/fstab 
# mkdir /raid5 ; mount /raid5 

■ RAID 5 구성 해제 절차
(ㄱ) 언마운트
# vi /etc/fstab 
# umount /raid5 
(ㄴ) RAID 구성 삭제
# mdadm --stop /dev/md0 
# mdadm --remove /dev/md0 
# mdadm --zero-superblock /dev/sdc1 /dev/sdd1 /dev/sde1 
# rm /etc/mdadm.conf
```

① RAID 5 구성
[TERM1] 모니터링 윈도우

```bash

# watch cat /proc/mdstat    (# watch CMD)
Every 2.0s: cat /proc/mdstat                                                Wed Jan 22 17:49:32 2014

Personalities : [raid6] [raid5] [raid4] [raid0] [raid1]
md0 : active raid5 sde1[3] sdd1[1] sdc1[0]
      2088192 blocks level 5, 64k chunk, algorithm 2 [3/2] [UU_]
      [=>...................]  recovery =  9.3% (97920/1044096) finish=0.8min speed=19584K/sec

unused devices: <none>

Every 2.0s: cat /proc/mdstat                                                Wed Jan 22 17:50:12 2014

Personalities : [raid6] [raid5] [raid4] [raid0] [raid1]
md0 : active raid5 sde1[2] sdd1[1] sdc1[0]
      2088192 blocks level 5, 64k chunk, algorithm 2 [3/3] [UUU]

unused devices: <none>

```

[TERM2] 명령어 윈도우

```bash
# mdadm --create /dev/md0 --level=5 --raid-device=3 /dev/sdc1 /dev/sdd1 /dev/sde1
mdadm: /dev/sdc1 appears to contain an ext2fs file system
    size=524160K  mtime=Thu Jan 28 08:36:48 2010
mdadm: /dev/sdc1 appears to be part of a raid array:
    level=raid1 devices=2 ctime=Thu Jan 28 08:49:57 2010
mdadm: /dev/sdd1 appears to contain an ext2fs file system
    size=524160K  mtime=Thu Jan 28 08:36:48 2010
mdadm: /dev/sdd1 appears to be part of a raid array:
    level=raid1 devices=2 ctime=Thu Jan 28 08:49:57 2010
Continue creating array? y  
mdadm: array /dev/md0 started.

# mdadm --detail /dev/md0
/dev/md0:
        Version : 0.90
  Creation Time : Sun Apr 29 17:45:16 2012
     Raid Level : raid5
     Array Size : 2088192 (2039.59 MiB 2138.31 MB)
  Used Dev Size : 1044096 (1019.80 MiB 1069.15 MB)
   Raid Devices : 3
  Total Devices : 3
Preferred Minor : 0
    Persistence : Superblock is persistent

    Update Time : Sun Apr 29 17:45:16 2012
          State : clean, degraded, recovering
 Active Devices : 2
Working Devices : 3
 Failed Devices : 0
  Spare Devices : 1

         Layout : left-symmetric
     Chunk Size : 64K

 Rebuild Status : 81% complete

           UUID : 4bf1e377:5469adc8:b02500fb:a355e047
         Events : 0.1

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       49        1      active sync   /dev/sdd1
       3       8       65        2      spare rebuilding   /dev/sde1

.... (약간 시간이 흐른 후) ....

# mdadm --detail /dev/md0
/dev/md0:
        Version : 0.90
  Creation Time : Sun Apr 29 17:45:16 2012
     Raid Level : raid5
     Array Size : 2088192 (2039.59 MiB 2138.31 MB)
  Used Dev Size : 1044096 (1019.80 MiB 1069.15 MB)
   Raid Devices : 3
  Total Devices : 3
Preferred Minor : 0
    Persistence : Superblock is persistent

    Update Time : Sun Apr 29 17:45:54 2012
          State : clean
 Active Devices : 3
Working Devices : 3
 Failed Devices : 0
  Spare Devices : 0

         Layout : left-symmetric
     Chunk Size : 64K

           UUID : 4bf1e377:5469adc8:b02500fb:a355e047
         Events : 0.2

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       49        1      active sync   /dev/sdd1
       2       8       65        2      active sync   /dev/sde1
② /etc/mdadm.conf 파일 설정
# echo "DEVICE partitions" > /etc/mdadm.conf 
# mdadm --detail --scan >> /etc/mdadm.conf 
# cat /etc/mdadm.conf
DEVICE partitions
ARRAY /dev/md0 level=raid5 num-devices=3 metadata=0.90 UUID=4bf1e377:5469adc8:b02500fb:a355e047
③ 파일시스템 생성
# mkfs.ext4 /dev/md0
mke2fs 1.39 (29-May-2006)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)

.... (중략) .....

Writing inode tables: done
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done

This filesystem will be automatically checked every 36 mounts or
180 days, whichever comes first.  Use tune2fs -c or -i to override.
# tune2fs -l /dev/md0 
-> 출력 내용 확인
④ 마운트 작업
# mkdir -p /raid5 
# mount /dev/md0 /raid5 
# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1              17G  3.4G   12G  22% /
/dev/sda8             487M   11M  451M   3% /data1
/dev/sda7             487M   11M  451M   3% /data2
/dev/sda6             487M   11M  451M   3% /data3
/dev/sda5             487M   11M  451M   3% /data4
/dev/sda3             487M   11M  451M   3% /home
tmpfs                 506M     0  506M   0% /dev/shm
/dev/md0              2.0G   36M  1.9G   2% /raid5
# vi /etc/fstab
-------------------------------
..... (중략) .....
#
# (4) RAID Configuration
#
#/dev/md0               /raid0                  ext4    defaults        1 2
#/dev/md0               /raid1                  ext4    defaults        1 2
/dev/md0                /raid5                  ext4    defaults        1 2
--------------------------------
⑤ 언마운트
# cd 
# umount /raid5 
# vi /etc/fstab
--------------------------------
..... (중략) .....
#
# (4) RAID Configuration
#
#/dev/md0               /raid0                  ext4    defaults        1 2
#/dev/md0               /raid1                  ext4    defaults        1 2
#/dev/md0                /raid5                  ext4    defaults       1 2
--------------------------------
⑥ RAID 설정 삭제
# mdadm --stop /dev/md0
mdadm: stopped /dev/md0
# mdadm --remove /dev/md0
# mdadm --zero-superblock /dev/sdc1 /dev/sdd1 /dev/sde1

# rm -f /etc/mdadm.conf 
#
```

### [EX5] 장애 디스크 교체 과정(시스템 종료없이 디스크 교체)

```bash
RAID 1 / RAID 5 구성된 장치에 대해서는 온라인상에서 장애 디스크 교체가 가능하다.(RAID 1) # mdadm --create /dev/md0 --level=1 --raid-device=2 /dev/sdc1 /dev/sdd1(RAID 5) # mdadm --create /dev/md0 --level=5 --raid-device=3 /dev/sdc1 /dev/sdd1 /dev/sde1
아래 실습에서는 RAID 5 가지고 실습을 하였다.

① RAID 5 구성
# mdadm --create /dev/md0 --level=5 --raid-device=3 /dev/sdc1 /dev/sdd1 /dev/sde1
mdadm: /dev/sdc1 appears to contain an ext2fs file system
    size=2088192K  mtime=Sun Apr 29 17:50:00 2012
mdadm: /dev/sdc1 appears to be part of a raid array:
    level=raid5 devices=3 ctime=Sun Apr 29 17:45:16 2012
mdadm: /dev/sdd1 appears to be part of a raid array:
    level=raid5 devices=3 ctime=Sun Apr 29 17:45:16 2012
mdadm: /dev/sde1 appears to contain an ext2fs file system
    size=2088192K  mtime=Sun Apr 29 17:50:00 2012
mdadm: /dev/sde1 appears to be part of a raid array:
    level=raid5 devices=3 ctime=Sun Apr 29 17:45:16 2012
Continue creating array? y
mdadm: array /dev/md0 started.
-> 임시적인 테스트이기 때문에 /etc/mdadm.conf 파일에 정의하지는 않는다.

		[참고] watch CMD  (# watch CMD)
		# watch cat /proc/mdstat 
		<CTRL + C>

② 파일시스템 생성
# mkfs.ext4 /dev/md0
mke2fs 1.39 (29-May-2006)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
261120 inodes, 522048 blocks
26102 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=536870912
16 block groups
32768 blocks per group, 32768 fragments per group
16320 inodes per group
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912

Writing inode tables: done                            
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done

This filesystem will be automatically checked every 34 mounts or
180 days, whichever comes first.  Use tune2fs -c or -i to override.
③ 마운트 작업 및 RAID 5(EX: /dev/md0) 상태 정보 확인
# mount /dev/md0 /raid5 
-> 실습에서는 /etc/fstab 정의하지는 않는다.

# df –h /raid5
Filesystem    Type    Size  Used Avail Use% Mounted on
/dev/md0      ext4    2.0G   36M  1.9G   2% /raid5
-> 마운트 확인

		(필요하면 명령어 수행)
		# cd /raid5 
		# cp /etc/passwd file1 
		# cp /etc/group file2 
		# ls 
		# cd
# mdadm --detail /dev/md0
/dev/md0:
        Version : 0.90
  Creation Time : Sat May 12 10:33:07 2012
     Raid Level : raid5
     Array Size : 2088192 (2039.59 MiB 2138.31 MB)
  Used Dev Size : 1044096 (1019.80 MiB 1069.15 MB)
   Raid Devices : 3
  Total Devices : 3
Preferred Minor : 0
    Persistence : Superblock is persistent

    Update Time : Sat May 12 10:34:06 2012
          State : clean
 Active Devices : 3
Working Devices : 3
 Failed Devices : 0
  Spare Devices : 0

         Layout : left-symmetric
     Chunk Size : 64K

           UUID : 9d77d247:131ed99b:c965729f:87938a9b
         Events : 0.2

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       49        1      active sync   /dev/sdd1
       2       8       65        2      active sync   /dev/sde1
-> 정보 확인

④ 장애가 발생한 디스크를 fault(fail) 상태로 설정 및 확인
# mdadm /dev/md0 -f /dev/sdd1      /* faulty : 결함 있는 상태로 설정 */
mdadm: set /dev/sdd1 faulty in /dev/md0
		[참고] 명령어 옵션 비교
		# mdadm /dev/md0 --fail /dev/sdd1 
		# mdadm /dev/md0 -f /dev/sdd1 
		
# mdadm --detail /dev/md0 | tail
UUID : 366a9619:ed283577:108041dc:050b88cb
         Events : 0.6

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       0        0        1      removed                   /* 장치가 지워진 상태 */
       2       8       65        2      active sync   /dev/sde1

       3       8       49        -      faulty spare   /dev/sdd1  /* 결함 상태로 표시 */
		(필요하면 명령어 수행)
		# touch /raid5/file3 
		# ls /raid5 

⑤ 장애가 발생한 디스크를 remove 및 확인
# mdadm /dev/md0 -r /dev/sdd1 /* removed : 장치를 제거 함 */
mdadm: hot removed /dev/sdd1
		[참고] 명령어 옵션 비교
		# mdadm /dev/md0 --remove /dev/sdd1
		# mdadm /dev/md0 -r /dev/sdd1

# mdadm --detail /dev/md0 | tail
UUID : 366a9619:ed283577:108041dc:050b88cb
         Events : 0.8

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       0        0        1      removed                  /* 장치가 지워진 상태 */
       2       8       65        2      active sync   /dev/sde1
		(필요하면 명령어 수행)
		# touch /raid5/file4 
		# ls /raid5
⑥ 장애가 발생한 디스크를 새로운 디스크로 교체 및 확인
# mdadm /dev/md0 -a /dev/sdd1 /* add : 장치를 추가 함 */
mdadm: added /dev/sdd1
		[참고] 명령어 옵션 비교
		# mdadm /dev/md0 --add /dev/sdd1
		# mdadm /dev/md0 -a dev/sdd1

# mdadm --detail /dev/md0 | tail
Rebuild Status : 28% complete

           UUID : aae7137b:a49d0b43:05d20e8f:02a22cbe
         Events : 0.6

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       3       8       49        1      spare rebuilding   /dev/sdd1
       2       8       65        2      active sync   /dev/sde1
# mdadm --detail /dev/md0
Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       49        1      active sync   /dev/sdd1
       2       8       65        2      active sync   /dev/sde1
⑦ RAID 5 삭제
# cd 
# umount /raid5 
# mdadm --stop /dev/md0 
# mdadm --remove /dev/md0 
# mdadm --zero-superblock /dev/sdc1 /dev/sdd1 /dev/sde1
```

### [EX] 추가적인 실습

```bash
(ㄱ) 디스크 준비
/dev/sdc1(1G), /dev/sdd1(1G), /dev/sde1(1G), /dev/sdf1(1G), 
/dev/sdg1(1G), /dev/sdh1(1G), /dev/sdi1(1G)

(ㄴ) RAID 구현 절차
RAID 구성(mdadm --create) -> F/S(mkfs.ext4) -> Mount(mount,/etc/fstab)
          /etc/mdadm.conf

(ㄷ) 작업 시나리오
RAID 0 (/dev/md0) - /dev/sdc1, /dev/sdd1            => /raid0
RAID 1 (/dev/md1) - /dev/sde1, /dev/sdf1            => /raid1
RAID 5 (/dev/md5) - /dev/sdg1, /dev/sdh1, /dev/sdi1 => /raid5

# time dd if=/dev/zero of=/test/file1  bs=500M count=1 
# time dd if=/dev/zero of=/raid0/file1 bs=500M count=1 
# time dd if=/dev/zero of=/raid1/file1 bs=500M count=1 
# time dd if=/dev/zero of=/raid5/file1 bs=500M count=1 

(ㄹ) RAID 삭제
Umount(umount, /etc/fstab) -> RAID 구성 삭제(mdadm --stop)
                                            /etc/mdadm.conf

(ㅁ) 참고
# cat /proc/mdstat 
# mdadm —detail /dev/md0 
# ls –l /etc/mdadm.conf
```

```bash
--------------------------------- 작업 과정 ------------------------------------

(ㄱ) 디스크 준비
# chkconfig kudzu on 
# poweroff 
새로운 디스크 장착(EX: /dev/sdi)
Power ON
# ls -l /dev/sd?  (# fdisk -l | grep 'Disk /dev')

# fdisk /dev/sdc 
# fdisk /dev/sdd 
# fdisk /dev/sde 
# fdisk /dev/sdf 
# fdisk /dev/sdg 
# fdisk /dev/sdh 
# fdisk /dev/sdi 
# fdisk -l | grep raid  (# fdisk –l /dev/sd? | grep raid)

(ㄴ) RAID 구성 작업
# mdadm --create /dev/md0 --level=0 --raid-device=2 /dev/sdc1 /dev/sdd1 
# mdadm --create /dev/md1 --level=1 --raid-device=2 /dev/sde1 /dev/sdf1 
# mdadm --create /dev/md5 --level=5 --raid-device=3 /dev/sdg1 /dev/sdh1 /dev/sdi1 
# cat /proc/mdstat  (# watch cat /proc/mdstat)

# echo "DEVICE partitions" > /etc/mdadm.conf 
# mdadm --detail --scan >> /etc/mdadm.conf 
# cat /etc/mdadm.conf 

(ㄷ) 파일시스템 작업
# mkfs.ext4 /dev/md0 
# mkfs.ext4 /dev/md1 
# mkfs.ext4 /dev/md5 
# tune2fs –l /dev/md0 
# tune2fs –l /dev/md1 
# tune2fs -l /dev/md5 

(ㄹ) 마운트 작업
# mkdir -p /raid0 /raid1 /raid5 
# vi /etc/fstab 
# mount /raid0 
# mount /raid1 
# mount /raid5 
# df -h 

(ㅁ) 파일생성 테스트
# time dd if=/dev/zero of=/test/file1  bs=500M count=1 (0m5.689s:0m7.838s:0m6.675s:0m6.572s
# time dd if=/dev/zero of=/raid0/file1 bs=500M count=1 (0m3.325s:0m5.658s:0m8.190s:0m6.631s
# time dd if=/dev/zero of=/raid1/file1 bs=500M count=1 (0m2.314s:0m6.844s:0m9.022s:0m7.101s
# time dd if=/dev/zero of=/raid5/file1 bs=500M count=1 (0m1.925s:0m1.982s:0m7.252s:0m3.982s

(ㅂ) RAID 삭제
# umount /raid0 
# umount /raid1 
# umount /raid5 
# vi /etc/fstab 
# df -h 

# mdadm --stop /dev/md0 
# mdadm --stop /dev/md1 
# mdadm --stop /dev/md5 
# cat /proc/mdstat 

# mdadm --remove /dev/md0 
# mdadm --remove /dev/md1 
# mdadm --remove /dev/md5 

# mdadm --zero-superblock /dev/sdc1 /dev/sdd1 
# mdadm --zero-superblock /dev/sde1 /dev/sdf1 
# mdadm --zero-superblock /dev/sdg1 /dev/sdh1 /dev/sdi1 

# rm -f /etc/mdadm.conf 
# ls -l /etc/mdadm.conf 

--------------------------------- 작업 과정 ------------------------------------
```

■ RAID & LVM 비교

```bash
RAID

RAID 0 concatenate
RAID 0 stripe

RAID 1 Mirror

RAID 5 with parity(stripe)
```

```bash
LVM

LVM 기본 구성
LVM stripe 구성

LVM Mirror 구성

기능 지원 X
```

(실무 예) RAID/LVM 사용 예

```bash
■ 운영체제 디스크
			H/W RAID 구성(RAID Controller)

			----- H/W ---------- OS -----
			Disk1   ------+-----------> /dev/sda(적당하게 파티션 작업)
							      |
			Disk2   ------+

■ 데이터 디스크
			H/W RAID(Storage/Array) + LVM 구성

			----- Storage ---------- OS -----
			RAID 1 + 0(LUN) -----------> /dev/sd[bcdefg] --- LVM Stripe ---> 
			or
			RAID 5 / 6(LUN) -----------> /dev/sd[bcdefg] --- LVM Stripe --->
```
