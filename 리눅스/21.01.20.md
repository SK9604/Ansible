# 단원목표

---

- NFS 개요
- NFS 서버
- NFS 서버 실습

---

# NFS(Network File System) 개요

■ 분산 파일 시스템 : NFS(UNIX/LINUX), CIFS(WINDOWS)

■ NFS (Network File System)

NFS는 컴퓨터 사용자가 원격지 컴퓨터에 있는 파일을 마치 자신의 컴퓨터에 있는 것처럼 검색하고, 마음대로 저장하거나 수정하도록 해주는 클라이언트/서버형 응용프로그램이다. 사용자 시스템에는 NFS 클라이언트가 있어야하며, 다른 컴퓨터 (원격지의 컴퓨터)에는 NFS 서버가 설치되어 있어야 한다. 또한, 둘 모두 TCP/IP 프로토콜이 설치되어 있어야 하는데, 왜냐하면, NFS 서버와 클라이언트가 파일을 보내거나 수정하는 프로그램으로 TCP/IP를 사용하기 때문이다 (그러나, 초기버전의 NFS에서는 TCP 대신에 UDP가 사용되기도 한다).

NFS는 썬마이크로시스템즈에 의해 개발되었으며, 파일서버의 표준으로 정착되었다. 이 프로토콜은 컴퓨터들 간의 통신 방법으로서 RPC를 사용한다. 윈도우 95와 썬(Sun)의 Solstice Network Client와 같은 제품을 사용하는 일부 운영체계에 NFS를 설치할 수 있다.

NFS를 이용하여, 사용자나 시스템관리자는 파일시스템의 전부 또는 일부를 설치할 수 있다. 설치된(액세스할 수 있도록 지정된) 파일시스템은 각 사용자들의 권한에 따라 개개의 파일을 액세스할 수 있게된다.

NFS는 인터넷 기술이 가미된 WebNFS로 확장되었으며, 이 제품과 제시된 표준안은 현재 넷스케이프 커뮤니케이터 브라우저의 일부이다. WebNFS는 썬마이크로시스템즈가 웹 페이지와 다른 인터넷 파일들을 빠르게 액세스할 수 있는 방법이라고 믿고 있는 바로 그것을 제공한다.

현재 리눅스에서 사용되는 NFS 버전은 NFSv2, NFSv3, NFSv4 지원이 된다. 리눅스 2.4 커널에서 NFSv3 지원되기 시작했으며, 리눅스 커널 2.6에서 NFSv4 지원되기 시작했다.

NFSv4 자세한 내용은 다음 사이트에서 확인한다.(http://www.citi.umich.edu/projects/nfsv4)

```bash
패키지: nfs-utils

----- NFS Client -----		----- NFS Server -----

      mount 			                  share 

															# vi /etc/exports 
# mount S:/share /p 		      # exportfs -r 
----------------------		----------------------
```

# NFS(Network File System) 서버

■ NFS 서버 데몬

■ NFS 관련 파일

■ NFS 관련 명령어

■ NFS 서비스 제어

(1) NFS 서버 데몬

NFS 서버는 언제든지 클라이언트가 마운트 할 수 있도록 준비 되어 있어야 하며 NFS는 rpc.mountd, rpc.nfsd 두 데몬을 가지고 있다. rpc.mountd 데몬과 rpc.nfsd 데몬은 RPC 기반의 서비스이기 때문에, rpcbind(portmapper)가 반드시 떠 있어야만 사용이 가능하다.

```bash
----- NFS Client -----		----- NFS Server -----  nfs3

      mount			            share

      rpc.statd			   rpc.mountd
      rpc.lockd			   rpc.nfsd
								 		   rpc.rquotad
										   rpc.statd
										   rpc.lockd
```

```bash
● rpc.mountd		        NFS mount daemon
● rpc.nfsd		          NFS server process
● rpc.rquotad(rquotad) 	remote quota server
● rpc.statd		          NSM(Network Status Monitor) status monitor
● rpc.lockd		          start kernel lockd process
● portmapper		        DARPA port to RPC program number mapper
```

```bash
nfs

공유된 NFS 서버에 대한 요청을 처리하기 위한 NFS서버와 RPC 프로세스를 실행한다.

rpc.mountd
NFS에서 마운트 요청을 처리하기 위해 사용되는 데몬
	클라이언트에서 마운트 요청을 하는 경우 /etc/exports 파일에 등록이 되어 있는지 확인한다. 마운트 요청에 대하여 성공적으로 완료된다면 rpc.mountd는 성공상태로 응답 클라이언트는 공유에 대하여 파일 핸들을 제공한다.
rpc.nfsd
	nfs버전과 프로토콜이 정의되도록 하고 사용자 레벨을 제공한다. 주요기능은 nfsd에서 처리
rpc.rquotad(rquotad)
	원격 사용자에게 사용자 할당 정보 제공 ( 자동 시작이기 때문에 사용자 구성이 따로 필요하지는 않다.)
rpc.statd
rpc.lockd
	NFS서버의 파일을 클라이언트 측에서 잠글수 있도록 하는 서비스
portmapper
NFS4 

rpc.mountd
	export 설정을 위해 사용
nfs-server
	파일 시스템 사용을 위한 서비스
	rpc.nfsd , exportfs 프로세스가 실행
rpc.idmapd
	NFSv4 ID와 이름 연결
```

■ rpc.mountd : NFS 클라이언트의 마운트 요청에 응답하는 데몬.

■ rpc.nfsd : NFS 클라이언트가 마운트된 자원을 사용할 수 있도록 도와주는 데몬.

■ rpc.rquotad : Quotas 관리하는 데몬

■ rpc.statd

■ rpc.lockd

(2) NFS 관련 파일

■ /etc/exports **(NFS Server)**에서 부팅시에 공유하는 정보를 담는 파일

■ /etc/fstab **(NFS Client)**에서 부팅시에 마운트하는 정보를 담는 파일

(2-1) /etc/exports 파일

```bash
# cat /etc/exports 
	#
	# sample /etc/exports file
	#
	#/               master(rw) trusty(rw,no_root_squash)
	#/projects       proj*.local.domain(rw)
	#/usr            *.local.domain(ro) @trusted(rw)
	#/home/joe       pc001(rw,all_squash,anonuid=150,anongid=100)
	#/pub            (ro,insecure,all_squash)
	/backup         backupserver(rw,no_root_squash)
	/share          *(rw)

-> 기본 파일은 존재하지만 내용은 없는 상태이다.(파일의 크기가 '0' 이다.)
-> /etc/exports 파일 형식
   [공유디렉토리]	[접근가능 서버/네트워크] (공유옵션)
   # cat /etc/exports
   /share         172.16.9.254(rw)
-> [공유디렉토리]는 공유할려고 하는 디렉토리 이름이다.   (예) /share, /home, /export/home
-> [접근가능 서버/네트워크]는 호스트이름, IP 주소, DNS 도메인 이름이 가능하다.   
   (예) nfs.linux.co.kr, 172.16.9.252, *, *.example.com, 192.168.0.0/24, 192.168.0.0/255.255.255.0
-> (공유옵션)은 반드시 지정해야 한다.
```

■ /etc/exports 파일에서 사용 가능한 공유 옵션

```bash
옵 션             설 명
root_squash       클라이언트에서 root 사용자를 서버상에서는 nobody 사용자로 매핑한다.
**no_root_squash**    서버와 클라이언트 모두 같은 root를 사용한다. 즉, 클라이언트에서의 root 사용자의 요청을 서버의 root 사용자로 매핑한다.
**ro**                파일시스템을 읽기 전용(Read Only)로 마운트한다. (EX: ISO파일 같은 변경이 되지않는 파일, 백업의 복구용 파일들)
**rw**                파일시스템을 읽고 쓸수 있도록 마운트한다.(Read/Write)
insecure          인증되지 않은 접근도 가능하도록 한다.
link_relative     심볼릭 링크를 상대 심볼릭 링크로 바꿀 때 사용한다.
noaccess          지정된 디렉토리에는 접근을 금지한다. 특정시스템에 대한 공유 디렉토리 일부를 접근 못하게 할 경우에 사용한다.
anonuid, anongid  익명사용자(Anonymous/Nobody 사용자에 대한 UID/GID 번호 맞추기
```

```bash
■ /etc/exports 파일의 기본 설정 해석

/		master(rw) trusty(rw,no_root_squash)
● / 디렉토리를 master, trusty 호스트가 읽기/쓰기를 허용한다. trusty 호스트는 root 사용자를 인정해 준다.

/projects 	proj*.local.domain(rw)
● 도메인이름이 local.domain이고 호스트이름이 proj 로 시작하는 호스트에 대해서 /projects라는 디렉토리로 읽기/쓰기를 허용한다.

/projects 	project[0-20].local.domain(rw)
● 도메인이름이 local.domain이고 호스트이름이 pproject0번에서 project20번 까지의 호스트에 대해서 /projects라는 디렉토리로 읽기/쓰기를 허용한다.

/usr            	*.local.domain(ro) @trusted(rw)
● /usr 디렉토리를 *.local.domain 도메인에 대해서는 읽기전용(Read Only)으로 허용하고, @trusted 넷그룹이름을 가진 호스트에 대해서는 읽고/쓰기 권한으로 허용한다.

/data1		172.16.0.0/255.255.0.0(ro)
● 네트워크 주소가 172.16.0.0 이고 255.255.0.0 호스트에 대해서 /data1 디렉토리를 읽기만 허용한다.

/home/joe       	pc001(rw,all_squash,anonuid=150,anongid=100)
● /home/joe 디렉토리를 pc001 호스트에 대해서 읽기/쓰기 권한, 익명사용자(Anonymous User)로 사용자를 변환하고, 익명사용자의 UID는 150으로 익명사용자 GID는 100으로 설정한다.

/pub		*(ro,insecure,root_squash)
● /pub 디렉토리에 읽기전용으로 마운트할 수 있고, 인증없이 마운트가 가능하며 마운트하는 모든 컴퓨터의 루트를 서버에서 nobody로 접근할 수 있게 한다.

/backup         	backupserver(rw,no_root_squash)
● /backup 디렉토리는 backupserver 쪽에서 읽기/쓰기, root 사용자를 인정해 준다. 

(주의) /etc/exports 파일의 설정 차이점
# vi /etc/exports 
	/home	test.example.com(rw)
	/home	test.example.com  (rw)    /* /home    test.example.com(ro)  *(rw) */
	
-> 첫번째 예제는 /home에 대해서 test.example.com에서 rw 형태로 마운트 가능하다는 뜻이고,
-> 두번째 예제는 /home에 대해서 test.example.com에 대해서 read only(default)로 마운트가 가능하고 모든 호스트(world)에서 rw 형태로 마운트가 가능하다는 뜻이다.
```

(2-2) /etc/fstab 파일

```bash
The  file fstab contains descriptive information about the various file systems.
       fstab is only read by programs, and not written; It is the duty  of  the  system
       administrator  to  properly  create  and maintain this file.  Each filesystem is
       described on a separate line; fields on each  line  are  separated  by  tabs  or
       spaces.  Lines starting with ’#’ are comments.  The order of records in fstab is
       important because fsck(8), mount(8), and umount(8) sequentially iterate  through
       fstab doing their thing.
```

```bash
# cat /etc/fstab
LABEL=/                 /                       ext3    defaults        1 1
LABEL=/data1            /data1                  ext3    defaults        1 2
LABEL=/data2            /data2                  ext3    defaults        1 2
LABEL=/data3            /data3                  ext3    defaults        1 2
LABEL=/data4            /data4                  ext3    defaults        1 2
LABEL=/home             /home                   ext3    defaults        1 2
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
LABEL=SWAP-sda2         swap                    swap    defaults        0 0
#
# (1) NFS Test
#
nfs.linux.co.kr:/usr/local    /usr/local        nfs    timeo=15,intr    0 0
```

```bash
# mount -t nfs -o timeo=15,intr nfs.linux.co.kr:/usr/local /usr/local
```

```bash
■ /etc/fstab 파일에서 사용 가능한 마운트 옵션
옵 션     설 명
timeo=n   RPC 타임아웃이 발생되고 나서 첫번째 재전송 요구를 보낼때 사용되는 시간, 기본값은 7(1/10초)
intr      주 타임아웃이 발생되었을 때 신호를 보내 NFS 호출을 인터럽트한다.
rsize=n   NFS 서버로부터 읽어들이는 바이트 수 지정, 기본값은 1024Bytes
wsize=n   NFS 서버에 쓰기할 때 사용하는 바이트 수 지정, 기본값은 1024Bytes
retrans=n 주 타임아웃을 발생시키는 부 타임아웃을 재전송 회수 기본값은 3번의 타임아웃
port=n    NFS 서버와 연결할 수 있는 포트번호 지정
fg        첫번째 NFS 마운트 시도에서 타임아웃이 발생되면 즉시 중단함. 기본값
hard      주 타임아웃이 발생되면 "server not responding"을 출력하고 무한정 재시도
soft      주 타임아웃이 발생되면 프로그램에게 I/O 에러 보고

■ /etc/fstab 파일의 기본 파일 해석
nfs.linux.co.kr:/usr/local    /usr/local        nfs    timeo=15,intr    0 0

● nfs.linux.co.kr 서버의 /usr/local 디렉토리를 로컬시스템의 /usr/local 디렉토리에 NFS을 통해 마운트한다. 마운트시 timeout 시간은 15초, 중간에서 인터럽트(Interrupt) 걸수 있도록 설정 되어 있다.
```

[참고]

마운트가 완성된 클라이언트를 부팅하는 경우에는 항상 nfs 서버를 먼저 기동하고서 클라이언트를 부팅한다.

NFS 클라이언트 - 서버 구조

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/56ff36bf-2cd5-4dd5-b814-7ec103911000/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/56ff36bf-2cd5-4dd5-b814-7ec103911000/Untitled.png)

NFS가 클라이언트 - 서버 구조로 파일시스템을 제공하는 경우에 사용하는 동작 방식이다. 

클라이언트가 서버의 로컬 디스크를 네트워크를 통하여 마운트 한다.

클라이언트 시스템의 프로그램이 커널로부터 하드디스크, 프로세스 관련하여 접근을 하기 위해서는 VFS를 통하여 접근한다.

시스템 콜은 운영체제의 커널로 부터 하드디스크 접속, 프로세스 생성 같은 서비스를 요청하는 하나의 방법이다. 이것을 통해 인터페이스를 제공한다.

VFS는 (현재 시스템)리눅스에 존재하는 파일 시스템의 접근을 허용하기 위해 사용되는 인터페이스 이다.

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f7557f38-c2bd-4241-ac64-2aaa866fea6b/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f7557f38-c2bd-4241-ac64-2aaa866fea6b/Untitled.png)

프로그램이 NFS 파일 시스템을 사용하는 경우에는 VFS 인터페이스를 통하여 NFS 클라이언트로 접근하여 이 요청을 NFS서버에게 전달한다.

NFS를 사용하는 경우 NFS 인터페이스를 통해 NFS 클라이언트로 접근하여 NFS 서버로 전달하는 경우 NFS 클라이언트는 이 요청을 RPC/XDR 프로토콜로 전달한다. 이 요청에는 NFS 서버의 이름과 공유된 디렉토리의 정보를 같이 포함 하고 있다.

NFS 클라이언트는 L7 RPC L5 XDR L6에서 동작한다.

RPC는 컴퓨터의 프로그램이 다른 컴퓨터의 프로그램을 호출하여 두 프로그램 사이에서 직접 통신하는 프로토콜이다. 

XDR은 SUN에서 개발한 프로토콜로 이기종간의 컴퓨터 데이터 표현 방식이 다른 경우 XDR 형식으로 데이터를 변경하고 다른 컴퓨터에서 변환된 데이터를 다시 해당 컴퓨터에 맞도록 재 변환 하는 것을 도와주는 역할을 한다.

RPC는 TCP/IP 네트워크를 통해 클라이언트의 요청을 서버쪽으로 전달하고 서버는 실행결과를 다시 클라이언트로 반환한다. 분산처리 시스템에서 보통 많이 사용된다.

클라이언트에서 요청하는 프로그램이 서버의 결과 처리가 반환될 때까지 일시적으로 정지 상태에 이르는 동기식 방식을 사용한다. RPC는 rpcbind에 의하여 제어되고 있다.

RPC와 XDR은 TCP/IP를 통해 이 요청을 NFS 서버로 전달하고 NFS2, NFS3은 UDP, TCP중 선택사용하지만 NFSv4는 TCP를 사용하고 PORT는 2049번을 사용한다.

서버는 2049번으로 요청 받은 파일시스템에 접근하기 위하여 VFS 인터페이스를 통하여 파일시스템에 접근하고 rpc.mountd 서비스로 로컬 디스크를 NFS 클라이언트로 전달해 줌으로써 클라이언트 시스템이 서버의 디스크를 자신의 로컬 디스크인것 처험 사용할 수 있게 기능을 제공한다.

(3) NFS 관련 명령어

■ exportfs 명령어 **(NFS Server)** 공유자원을 확인 할 때 사용하는 명령어

■ showmount 명령어 **(NFS Client)** 공유할 자원 확인 할 때 사용하는 명령어

■ mount 명령어 **(NFS Client)** 공유된 자원 마운트 할 때 사용하는 명령어

(3-1) exportfs 명령어

● NFS 서버에서 공유된 자원을 확인 할 때 사용하는 명령어이다.

```bash
(명령어 형식)
# exportfs         /* 공유된 자원 정보를 간략하게 보여준다. */
# exportfs -v      /* 공유된 자원 정보를 자세하게 보여준다. */
# exportfs -ar     /* 공유된 자원 목록(예: /etc/exports)을 다시 읽는다.(reread) */
```

### [EX] exportfs 명령어 실습

> 사전 작업

```
SERVER3에 DISK 두장 추가 
용량 : 10G
현재 디스크가 인식이 되어 확인이 가능하다는 가정하에 진행
```

**[SERVER3]# ls -l /dev/sd***

```
brw-rw----. 1 root disk 8,  0  7월 28 23:57 /dev/sda
brw-rw----. 1 root disk 8,  1  7월 28 23:57 /dev/sda1
brw-rw----. 1 root disk 8,  2  7월 28 23:57 /dev/sda2
brw-rw----. 1 root disk 8,  3  7월 28 23:57 /dev/sda3
brw-rw----. 1 root disk 8,  4  7월 28 23:57 /dev/sda4
brw-rw----. 1 root disk 8,  5  7월 28 23:57 /dev/sda5
**brw-rw----. 1 root disk 8, 16  7월 28 23:57 /dev/sdb
brw-rw----. 1 root disk 8, 32  7월 28 23:57 /dev/sdc**
```

**[SERVER3]# fdisk /dev/sdb**

**[SERVER3]# fdisk /dev/sdc**

fdisk를 통하여 디스크 전체 용량으로 하나의 파티션을 지정한다.

**[SERVER3]# ls -l /dev/sdb***

```
brw-rw----. 1 root disk 8, 16  7월 29 00:00 /dev/sdb
brw-rw----. 1 root disk 8, 17  7월 29 00:00 /dev/sdb1
```

**[SERVER3]# ls -l /dev/sdc***

```
brw-rw----. 1 root disk 8, 32  7월 29 00:01 /dev/sdc
brw-rw----. 1 root disk 8, 33  7월 29 00:01 /dev/sdc1
```

**[SERVER3]# lsblk**

```
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0  100G  0 disk 
├─sda1   8:1    0    1G  0 part /boot
├─sda2   8:2    0  7.9G  0 part [SWAP]
├─sda3   8:3    0    5G  0 part /home
├─sda4   8:4    0    1K  0 part 
└─sda5   8:5    0 86.1G  0 part /
sdb      8:16   0   10G  0 disk 
└─**sdb1   8:17   0   10G  0 part**
sdc      8:32   0   10G  0 disk 
└─**sdc1   8:33   0   10G  0 part**
sr0     11:0    1 1024M  0 rom
```

**[SERVER3]# mkfs.xfs /dev/sdb1**

**[SERVER3]# mkfs.xfs /dev/sdc1**

**[SERVER3]# blkid**

```
/dev/sda1: UUID="dfe1e4b1-295c-4344-9781-291f7d9478e7" TYPE="ext4" PARTUUID="8aef5e73-01"
/dev/sda2: UUID="9bc56f33-11ab-46fb-8096-78a14f302fdc" TYPE="swap" PARTUUID="8aef5e73-02"
/dev/sda3: UUID="d0b93436-b57f-4b3c-ad6e-e2becd3dbb3e" TYPE="xfs" PARTUUID="8aef5e73-03"
/dev/sda5: UUID="dd42ca00-ec0c-4659-b096-ae21639703d4" TYPE="xfs" PARTUUID="8aef5e73-05"
**/dev/sdb1**: UUID="ffd20956-956a-41cf-b2d5-71b4f1aa1616" TYPE="xfs" PARTUUID="2c1175db-01"
**/dev/sdc1**: UUID="b6baa978-42b3-4cbb-9974-1af2b0cab68e" TYPE="xfs" PARTUUID="572d7a89-01"
```

**[SERVER3]# mkdir /stage1**

**[SERVER3]# mkdir /stage2**

**[SERVER3]# mount /dev/sdb1 /stage1**

**[SERVER3]# mount /dev/sdc1 /stage2**

**[SERVER3]# df | grep stage**

```
/dev/sdb1       10474496  106088  10368408   2% **/stage1**
/dev/sdc1       10474496  106088  10368408   2% **/stage2**
```

**[SERVER3]# umount /stage1**

**[SERVER3]# umount /stage2**

**[SERVER3]# vi /etc/fstab**

```
UUID=ffd20956-956a-41cf-b2d5-71b4f1aa1616 /stage1                 xfs     defaults        0 0
UUID=b6baa978-42b3-4cbb-9974-1af2b0cab68e /stage2                 xfs     defaults        0 0
```

**[SERVER3]# mount -a**

**[SERVER3]# df**

```
Filesystem     1K-blocks    Used Available Use% Mounted on
devtmpfs          987592       0    987592   0% /dev
tmpfs            1003532       0   1003532   0% /dev/shm
tmpfs            1003532    8928    994604   1% /run
tmpfs            1003532       0   1003532   0% /sys/fs/cgroup
/dev/sda5       90248136 2448792  87799344   3% /
/dev/sda3        5232640   69556   5163084   2% /home
/dev/sda1         999320  151288    779220  17% /boot
tmpfs             200704       0    200704   0% /run/user/0
**/dev/sdb1       10474496  106088  10368408   2% /stage1
/dev/sdc1       10474496  106088  10368408   2% /stage2**
```

**[SERVER3]# dnf search nfs**

**[SERVER3]# dnf search rpc**

**[SERVER3]# dnf -y install nfs-utils rpcbind**

> NFS 설정

[SERVER3]# vi /etc/exports

```
/stage1 192.168.10.0/24(rw)
```

**[SERVER3]# systemctl start rpcbind**

**[SERVER3]# systemctl start nfs-server**

**[SERVER3]# exportfs -r**

**[SERVER3]# firewall-cmd --list-services**

```
cockpit dhcpv6-client ssh
```

**[SERVER3]# firewall-cmd --permanent --add-service=nfs**

**[SERVER3]# firewall-cmd --permanent --add-service=rpc-bind**

**[SERVER3]# firewall-cmd --permanent --add-service=mountd**

**[SERVER3]# firewall-cmd --reload**

**[SERVER3]# exportfs**

```
/stage1       	192.168.10.0/24
```

**[SERVER3]# exportfs -v**

```
/stage1       	192.168.10.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,root_squash,no_all_squash)
```

**[SERVER2]# dnf -y install nfs-utils**

**[SERVER3]# exportfs -ar**

현재 설정된 내역이나 변경된 내역(/etc/exports)을 다시 읽어들인다.

a : all

r : reread

**[SERVER3]# showmount -e**

```
Export list for server3.example.com:
/stage1 192.168.10.0/24
```

**[SERVER2]# showmount -e**

```
clnt_create: RPC: Program not registered
```

**[SERVER2]# showmount -e 192.168.10.220**

```
Export list for 192.168.10.220:
/stage1 192.168.10.0/24
```

nfs 서버의 관리자 계정이 있다면 이러한 방법도 가능하다.

```
ssh root@192.168.10.220 'showmount -e'
```

### [EX] showmount 명령어 실습

(3-3) mount 명령어

- NFS 서버의 공유된 자원을 마운트 할 때 사용하는 명령어이다.
- 현재 마운트 할 때는 mount 명령어를 사용하고 부팅시에 마운트하기 위해서는 /etc/fstab 파일을 사용한다.

```
(명령어 형식)
mount [-t nfs] nfs.linux.co.kr:/usr/local /usr/local
	mount [-t nfs] 192.168.10.XXX:/share /mnt/share
```

fstab에 등록하는 경우 sec에 들어가는 옵션

```bash
sec=값
none 익명 파일액세스 , 쓰기가 허용되는 경우에는 nfsnobody로 uid, gid가 할당
**sys  UID, GID가 표준 파일 권한 , 지정되지 않는 경우에는 기본값**
krb5 클라이언트가 커버로스를 사용하여 ID를 증명하면 표준 리눅스 파일 사용 권한이 적용된다.
krv5i 요청한 데이터가 변경되지 않도록 감력한 암호화 보장을 추가한다.
krb5p 모든 요청에 암호화를 추가, 네트워크상의 데이터 노출을 방지한다. 단 이 설정의 경우에는 성능상에 영향을 미친다.

```

### [EX] mount 명령어 실습

```
  ----- NFS Server -----		----- NFS Client -----
	     (server3)			          (server2)

	----------------------		----------------------
```

**[SERVER2]# mkdir /share1**

**[SERVER2]# mkdir /share2**

**[SERVER2]# showmount -e 192.168.10.220**

```
Export list for 192.168.10.220:
/stage1 192.168.10.0/24
```

**[SERVER2]# mount -t nfs 192.168.10.220:/stage1 /share1**

- 도메인으로 등록하기

    [SERVER1]# cd /var/named

    [SERVER1]# vi server.zone

    ```bash
    $TTL 4
    @       IN SOA  ns1.example.com. root.example.com. (
                                            43      ; serial
                                            1D      ; refresh
                                            1H      ; retry
                                            1W      ; expire
                                            3H )    ; minimum
            NS      @
            A       192.168.10.200
            AAAA    ::1
            IN      NS      ns1
    ns1     IN      A       192.168.10.200
    www     IN      A       192.168.10.200
    nfs     IN      A       192.168.10.220        < 라인 추가
    ```

    [SERVER1]# systemctl restart named-chroot

    [SERVER2]# mount -t nfs [nfs.example.com](http://nfs.example.com/):/stage1 /share1

    [SERVER2]# df

    ```bash
    Filesystem             1K-blocks    Used Available Use% Mounted on
    devtmpfs                  987592       0    987592   0% /dev
    tmpfs                    1003532       0   1003532   0% /dev/shm
    tmpfs                    1003532    8928    994604   1% /run
    tmpfs                    1003532       0   1003532   0% /sys/fs/cgroup
    /dev/sda5               90248136 3530748  86717388   4% /
    /dev/sda3                5232640   69556   5163084   2% /home
    /dev/sda1                 999320  151288    779220  17% /boot
    tmpfs                     200704       0    200704   0% /run/user/0
    nfs.example.com**:/stage1  10474496  105984  10368512   2% /share1   <<<**
    ```

    [SERVER2]# cd /share1

    [SERVER2]# df .

    ```bash
    Filesystem             1K-blocks   Used Available Use% Mounted on
    nfs.example.com**:**/stage1  10474496 105984  10368512   2% /share1    <<<
    ```

**[SERVER2]# df**

```
Filesystem             1K-blocks    Used Available Use% Mounted on
devtmpfs                  987592       0    987592   0% /dev
tmpfs                    1003532       0   1003532   0% /dev/shm
tmpfs                    1003532    8928    994604   1% /run
tmpfs                    1003532       0   1003532   0% /sys/fs/cgroup
/dev/sda5               90248136 3530748  86717388   4% /
/dev/sda3                5232640   69556   5163084   2% /home
/dev/sda1                 999320  151288    779220  17% /boot
tmpfs                     200704       0    200704   0% /run/user/0
**192.168.10.220:/stage1  10474496  105984  10368512   2% /share1**
```

**[SERVER2]# cd /share1**

**[SERVER2]# df .**

```
Filesystem             1K-blocks   Used Available Use% Mounted on
192.168.10.220:/stage1  10474496 105984  10368512   2% /share1
```

**[SERVER2]# touch file1**

```
touch: cannot touch 'file1': 허가 거부
```

nfs SERVER —— nfs client

두 계정 모두 root이지만 client의 root 계정이라도 권한을 따로 지정하지 않으면 특수사용자로 접근

**[SERVER2]# ls -ld .**

```
drwxr-xr-x. 2 **root root** 6  7월 29  2020 .
```

**[SERVER2]# cd** 

**[SERVER2]# umount /share1**

**[SERVER2]# chown nobody.nobody /share1**

[참고] 이전버전에서는 nfsnobody라는 계정을 사용하였지만 8버전에서는 nobody라는 계정을 사용한다.

**[SERVER2]# chown nfsnobody.nfsnobody /share1**

**[SERVER2]# ls -ld /share1**

```
drwxr-xr-x. 2 nobody nobody 6  7월 28 16:06 /share1
```

**[SERVER3]# chmod 777 /stage1**

**[SERVER2]# mount -t nfs 192.168.10.220:/stage1 /share1**

**[SERVER2]# cd /share1**

**[SERVER2]# ls**

**[SERVER2]# touch file1**

**[SERVER2]# ls -l**

```
합계 0
-rw-r--r--. 1 nobody nobody 0  7월 29  2020 file1
```

**[SERVER2]# su - team01**

**[team01@server2 ~]$ cd /share1**

**[team01@server2 share1]$ ls**

**[team01@server2 share1]$ touch teamfile1**

**[team01@server2 share1]$ ls -l teamfile1**

```
-rw-rw-r--. 1 team01 team01 0 Jul 29  2020 teamfile1
```

> 추가 디스크 연결

**[SERVER3]# vi /etc/exports**

```
/stage1 192.168.10.0/24(rw)
/stage2 192.168.10.0/24(rw,no_root_squash)
```

NFS 서버에서 NFS 클라이언트의 root 사용자를 인정할 때 사용하는 공유 옵션이다.

**[SERVER3]# exportfs -ar**

**[SERVER2]# mount -t nfs 192.168.10.220:/stage2 /share2**

**[SERVER2]# cd /share2**

**[SERVER2]# ls**

**[SERVER2]# touch file1**

**[SERVER2]# ls -l**

```
합계 0
-rw-r--r--. 1 root root 0  7월 29  2020 file1
```

**[SERVER2]# reboot**

**[SERVER2]# vi /etc/fstab**

```
192.168.10.220:/stage1                    /share1                 nfs     defaults        0 0
192.168.10.220:/stage2                    /share2                 nfs     defaults        0 0
```

**[SERVER2]# mount -a**

**[SERVER2]# df**

```
[SERVER2]# mount -a
[SERVER2]# df
Filesystem             1K-blocks    Used Available Use% Mounted on
devtmpfs                  987592       0    987592   0% /dev
tmpfs                    1003532       0   1003532   0% /dev/shm
tmpfs                    1003532    8924    994608   1% /run
tmpfs                    1003532       0   1003532   0% /sys/fs/cgroup
/dev/sda5               90248136 3504936  86743200   4% /
/dev/sda3                5232640   69604   5163036   2% /home
/dev/sda1                 999320  151288    779220  17% /boot
tmpfs                     200704       0    200704   0% /run/user/0
192.168.10.220:/stage1  10474496  105984  10368512   2% /share1
192.168.10.220:/stage2  10474496  105984  10368512   2% /share2
```

💡 NFS 서버가 중지된 상태에서 클라이언트가 접속하면 생기는 문제

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/da150e8e-30a3-4953-aaa2-45bc7ef86e57/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/da150e8e-30a3-4953-aaa2-45bc7ef86e57/Untitled.png)

```bash
부팅중 문제가 생긴다.
완전 부팅이 안된 상태이고
mount -a 시 무한 대기상태이다.
반드시 NFS 서버를 먼저 작동시킨 뒤
client를 작동해야 한다.
```

## AutoFS

자동 마운트를 하는 이유

NFS 공유를 자동으로 마운트하고, 더 이상 사용하지 않는 경우 NFS 공유를 자동으로 마운트 해제하는 서비스(autofs)입니다.

### automount의 장점

mount 및 umount 명령을 실행하는 데 루트 권한이 필요 없습니다.

자동 마운트에 구성된 NFS 공유는 액세스 권한에 따라 시스템의 모든 사용자가 사용 할 수 있습니다.

**NFS 공유는 /etc/fstab의 항목과 같이 영구적으로 연결되지 않으므로 네트워크 및 시스템 리소스를 확보 해줍니다.**

자동 마운터는 **클라이언트측에 구성**되며 서버측 구성이 필요하지 않습니다.

자동 마운터는 **보안 옵션을 포함**하여 mount 명령과 동일한 옵션을 사용합니다.

자동 마운터는 마운트 지점 위치의 유연성을 위해 **직접 및 간접 마운트 지점 매핑을** 둘 다 지원합니다.

autofs는 간접 마운트 지점을 생성하고 제거하여 수동으로 관리할 필요가 없습니다.

기본 자동 마운터 네트워크 파일 시스템은 NFS지만 기타 네트워크 파일 시스템도 자동으로 마운트 할 수 있습니다.

autofs는 다른 시스템 서비스와 같이 관리되는 서비스입니다.

> automount 실습

```bash
마스터 맵 : 직접 맵 파일과, 간접 맵 파일을 정의하는 형식이 들어있는 파일
직접 맵 : 단독으로 하나씩 등록하는 파일 시스템
간접 맵 : 중첩하여 여러개 등록되는 파일 시스템
```

사전 조건 - share1 share2가 마운트가 되어 있다면 해제 해주어야 한다. 그리고 fstab 파일에 등록이 되어 있다면 주석 처리 해주어야 한다.

automount 작업은 위에서 확인 하였듯이 클아이언트에서 작업해야 한다. 혹시 마운트 되어있다면 모두 언마운트 후 /etc/fstab파일에서도 삭제한 뒤 진행

**[SERVER2]# dnf search autofs**

**[SERVER2]# dnf -y install autofs**

**[SERVER2]# cat /etc/auto.master**

```bash
#
# Sample auto.master file
# This is a 'master' automounter map and it has the following format:
# mount-point [map-type[,format]:]map [options]
# For details of the format look at auto.master(5).
#
/misc	/etc/auto.misc
#
# NOTE: mounts done from a hosts map will be mounted with the
#	"nosuid" and "nodev" options unless the "suid" and "dev"
#	options are explicitly given.
#
/net	-hosts
#
# Include /etc/auto.master.d/*.autofs
# The included files must conform to the format of this file.
#
+dir:/etc/auto.master.d
#
# If you have fedfs set up and the related binaries, either
# built as part of autofs or installed from another package,
# uncomment this line to use the fedfs program map to access
# your fedfs mounts.
#/nfs4  /usr/sbin/fedfs-map-nfs4 nobind
#
# Include central master map if it can be found using
# nsswitch sources.
#
# Note that if there are entries for /net or /misc (as
# above) in the included master map any keys that are the
# same will not be seen as the first read key seen takes
# precedence.
#
+auto.master
```

**[SERVER2]# cd /etc/auto.master.d/**

**[SERVER2]# pwd**

마스터 맵 파일을 추가하여 실제 마운트에 적용을 시켜 보자.

마스터 맵의 이름은 임의적으로 사용 가능 하지만 확장자는 항상 .autofs로 끝나야 한다.  단일 파일에 여러 위치들을 생성 가능 하다.

**[SERVER2]# vi stage.autofs**

```bash
/-      /etc/auto.direct    # -특수키 = 해당 구문에 참조되는 파일은 직접 맵 파일이다.
														# /- = 모든 내용이 /etc/auto.direct에 담겨 있다.
```

맵 파일에 대한 정의 파일을 생성하였고 실제 맵 파일을 구성하자.

**[SERVER2]# vi /etc/auto.direct**

```
/share1 -rw,sync        192.168.10.220:/stage1
```

**[SERVER2]# systemctl start autofs**

**[SERVER2]# systemctl status autofs**

**[SERVER2]# systemctl enable autofs**

**[SERVER2]# cd /share1**

**[SERVER2]# df .**

```
Filesystem             1K-blocks   Used Available Use% Mounted on
192.168.10.220:/stage1  10474496 105984  10368512   2% /share1
```

**[SERVER3]# cd /stage2**

테스트용 디렉토리를 1000개 정도 생성한다.

**[SERVER3]# mkdir dir{1..1000}**

**[SERVER2]# cd /etc/auto.master.d/**

**[SERVER2]# vi stage.autofs**

```
/-      /etc/auto.direct
**/share2 /etc/auto.indirect**
```

**[SERVER2]# vi /etc/auto.indirect**

```
* -rw,sync 192.168.10.220:/stage2/&
```

**[SERVER2]# systemctl restart autofs**

**[SERVER2]# cd /share2/dir2**

**[SERVER2]# df**

```
Filesystem                  1K-blocks    Used Available Use% Mounted on
devtmpfs                       987592       0    987592   0% /dev
tmpfs                         1003532       0   1003532   0% /dev/shm
tmpfs                         1003532    8956    994576   1% /run
tmpfs                         1003532       0   1003532   0% /sys/fs/cgroup
/dev/sda5                    90248136 3511000  86737136   4% /
/dev/sda3                     5232640   69604   5163036   2% /home
/dev/sda1                      999320  151288    779220  17% /boot
tmpfs                          200704       0    200704   0% /run/user/0
192.168.10.220:/stage2/dir2  10474496  106496  10368000   2% /share2/dir2
```

**[SERVER2]# df .**

```
Filesystem                  1K-blocks   Used Available Use% Mounted on
192.168.10.220:/stage2/dir2  10474496 106496  10368000   2% /share2/dir2
```

# ISCSI 개념

## ISCSI 소개

scsi [small computer system interface]를 IP 네트워크를 통하여 사용할 수 있는 개념

iscsi는  IP 네트워크를 통하여 SCSI의 고성능 스토리지 버스를 에뮬레이트 하기 위하여 사용하는 TCP/IP 기반의 프로토콜

기존의 SAN 프로토콜인 SCSI는 근거리 통신망으로서 짧은 거리의 네트워크 간에 사용을 하고 있지만 TCP/IP를 기반으로 사용하게 된다면 거리의 제약을 어느정도 없앨 수 있다.

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fea0571b-f765-4691-9e1c-dc4d18dee9a1/_2020-07-29__10.52.04.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fea0571b-f765-4691-9e1c-dc4d18dee9a1/_2020-07-29__10.52.04.png)

물리적으로 구성되는 FC는 가격 경쟁력에서 이더넷을 사용하는 ISCSI에 밀려 현재는 거의 사용되지 않는다. ISCSI에 대한 구현은 SCSI CDB명령어에 대한 집합을 TCP/IP 패킷내에 포함시켜 전송을 한다.

TCP/IP를 통하여 캡슐화된 iscsi 시스템 사이에 버스 통신을 수행하고 iscsi 서버는 파일이나 논리볼륨, 또는 물리적 디스크를 대상으로 표시된 기본 스토리지 계층인 (backstore)를 사용하여 SCSI 장치 처럼 에뮬레이트 해준다.

ISCSI 서비스는 일반적으로 운영체제에서 사용하는 TCP/IP 스택의 성능 향상을 위해 NIC카드 보다는 특수한 기능을 탑재한 TOE카드나 ISCSI HBA카드 사용을 선호한다.

기업내의 SAN을 사용하려면 전용 트래픽 인프라가 필요하다. 동일하게 iscsi를 사용하더라도 전용 인프라를 구성하는것이 좋다. 만약 WAN을 통하여 iscsi를 구성하는 경우에는 CHAP 인증 방식을 도입하여 한층 더 보안에 안정적인 구성을 할수 있다.

ISCSI는 보통 10/100 Mbps의 인프라가 아닌 최소 1G이상의 인프라를 구성해야 하며 실제 사용 용도로 구성하는 경우에는 4G 이상 추천은 10G 이더넷을 사용하는 것을 추천한다.

ISCSI를 사용하게 되면 케이블의 거리 제한을 넘어서 사용이 가능 하기 때문에 원격 데이터센터를 경유하는 데이터에 대하여 보다 용이하게 스토리지 통합을 구현 할수 있다. iscsi에 대한 구조는 추카적인 2차 장비가 없이도 구성이 가능하기 때문에 좀더 편리한 스토리지 구성이 가능하다.

그리고 원격 데이터센터와도 연결이 가능하기 때문에 데이터 복제, 마이그레이션, DR 구성등이 가능하다.

## ISCSI의 기본 용어

iscsi는 기본적인 서버-클라이언트 구조로 동작한다. 클라이언트 시스템의 경우에는 서버 스토리지를 대상으로 scsi 명령어를 전송하도록 해주는 initiator를 소프트웨어를 사용한다. 접속된 iscsi 대상 서버는 클라이언트 측에 포맷되지 않은 scsi 장치를 내보내주게 된다.

### 알고 있어야 하는 용어

---

initiator : 일반적으로는 소프트웨어도 가능하지만 ISCSI HBA 카드를 통해서도 구현이 가능하 iscsi 클라이언트이다. initiator는 고유한 이름을 지정해야 한다.(iqn)

target : iscsi 서버에서 연결을 할 수 있도록 구성된 iscsi 스토리지 리소스. 동일하게 고유한 이름을 지정해야 한다.(iqn) 대상은 논리 장치이기 때문에 순차적으로 적용된 번호가 존재 하고 이것을 넘버링 하여 블록 장치를 제공하고 있다.(lun) 하나의 iscsi 서버에서 여러개의 대상을 제공 가능 하다 

ACL : IQN 넘버를 사용하여 initiator데 대한 액세스 권한을 검증하는 액세스 제한 목록이다.

discovery : 대상 서버를 쿼리하여 구성된 대상을 나열 하는 경우 사용. 추가 적인 작업을 위해서는 login이 필요하다.

IQN :  초기자 및 대상을 식별하는데 사용하는 iSCSI 이름

IQN 이름 구성 방식 

**iqn.YYYY-MM.com.example:string**

iqn : 이 이름이 iqn이라는 것을 알리고 도메인 식별자, iscsi 초기자 이름으로 사용할 것을 알려준다.

YYYY-MM: 도메인 이름이 시작된 년도와 월을 작성한다.

com.example : 해당 기업 또는 도메인 소유자의 역방향 도메인

string: 해당 서버를 알수 있도록 작성되는 문자열

login : 클라이언트가 블록 장치를 사용한다는 것을 알리기 위해 LUN 또는 타겟서버 에게 인증을 받는것

LUN : 논리적인 장치 번호이다. 타겟서버에 연결되고 타겟서버를 통해 사용이 가능하고 번호가 매겨진 블록 장치를 나타낸다. 
           일반적으로는 하나의 대상이 하나의 LUN을 제공 하지만 특별한 경우에는 하나 이상의 LUN이 단일 대상에 연결되는 경우가 있다.

node : IQN으로 식별 가능한 iscsi 클라이언트 및 iscsi 서버

portal : 연결을 설정 할때 사용되는 타겟서버 또는 initiator의 IP주소 , 포트 
              일부 ISCSI 구현에서는 포털과 노드를 상호 교환하여 사용 가능하다.

TPG : 타겟 서버의 포털 그룹으로 ISCSI  대상이 수신 대기하는 TCP 포트와 인터페이스 IP주소에 대한 집합을 나타낸다. 대상에 대한 구성에 TPG를 추가하여 여러 LUN으로 배포하는 경우 설정에 대한 조정이 가능하다.

---

ISCSI는 ACL을 사용하여 LUN 마스킹을 수행하고 initiator에 대한 대상과 LUN에 대한 접근성을 관리한다. 대상에 대한 CHAP 인증을 제한 가능하다.

로컬 장치와는 다르게 ISCSI 네트워크 액세스 블록 장치는 여러 다양한 initiator에서 검색이 가능하다. 일반적인 파일 시스템의 경우에는 분산현 동시 다중 시스템 모니터링을 지원하지 않기 때문에 파일 시스템 손상에 취약하지만 클러스터 형태로 구성된 파일시스템을 사용하게 된다면 다중 시스템 액세스를 해결 가능하다.

## ISCSI 아키텍처

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/943dd0ec-602f-43c9-a48d-52ede50c3831/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/943dd0ec-602f-43c9-a48d-52ede50c3831/Untitled.png)
