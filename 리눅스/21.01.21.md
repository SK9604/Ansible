## ISCSI 구성

> ISCSI 서버 측 작업

선수작업으로 디스크 2장 연결

**[SERVER1]# dnf search iscsi**

**[SERVER1]# dnf search target**

**[SERVER1]# dnf -y install targetcli**

**[SERVER1]# rpm -ql targetcli**

**[SERVER1]# targetcli**

```
targetcli shell version 2.1.51
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.

/>
```

대화형 모드로 실행된 targetcli 

```
/> ls
o- / ..................................................................... [...]
  o- backstores .......................................................... [...]
  | o- block .............................................. [Storage Objects: 0]
  | o- fileio ............................................. [Storage Objects: 0]
  | o- pscsi .............................................. [Storage Objects: 0]
  | o- ramdisk ............................................ [Storage Objects: 0]
  o- iscsi ........................................................ [Targets: 0]
  o- loopback ..................................................... [Targets: 0]
```

초기 구성 상태

backsotre : 스토리지 종류에 대한 그룹, 디스크, file 등의 집합

**block : 서버에 정의 되어 있는 블록 장치이다. lsblk로 리스트되는 디스크, 파티션, 놀리볼륨, 멀티패스, 또다른 임의 장치중의 블록 디바이스들을 사용 가능**

fileio : 서버에서 지정된 파일 시스템내의 특정 파일을 가지고 사용할때 쓴다. 이 방법은 이미지 형태로된 가상 파일 시스템을 사용하는 것과 비슷하다.

pscsi : 물리적 scsi를 사용하는 경우에 사용되는 형식. 서버에 연결된 scsi 장치로의 통과를 허용해준다. 일반적으로는 사용되지 않는 형태

ramdisk : 메모리에 지정된 용량 만큼의 ramdisk 장치를 생성하여 사용하는 방식. 데이터가 영구적으로 저장 되지는 못한다. 

> 서버측 iscsi 구성 작업

블록 장치를 backstore에서 사용 및 구성 가능 하도록 정의

```
/> **/backstores/block create 192.168.10.200.disk1 /dev/sdb**
Created block storage object 192.168.10.200.disk1 using /dev/sdb.
```

iscsi에서 사용할 IQN 번호를 생성

```
/> **/iscsi create iqn.2020-07.com.example:192.168.10.200**
Created target iqn.2020-07.com.example:192.168.10.200.
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.
```

iscsi 타겟서버가 인증해줄 IQN도메인을 지정

```
/> **/iscsi/iqn.2020-07.com.example:192.168.10.200/tpg1/acls create iqn.2020-07.com.example:192.168.10.230**
Created Node ACL for iqn.2020-07.com.example:192.168.10.230
```

LUN 정보 등록

```
/> **/iscsi/iqn.2020-07.com.example:192.168.10.200/tpg1/luns create /backstores/block/192.168.10.200.disk1** 
Created LUN 0.
Created LUN 0->0 mapping in node ACL iqn.2020-07.com.example:192.168.10.230
```

기본 포탈 삭제

```
/> **cd /iscsi/iqn.2020-07.com.example:192.168.10.200/tpg1/portals/**
/iscsi/iqn.20.../tpg1/portals> **delete ip_address=0.0.0.0 ip_port=3260**
Deleted network portal 0.0.0.0:3260
/iscsi/iqn.20.../tpg1/portals> **ls**
o- portals ................................................................ [Portals: 0]
```

내보내기 할 포털 정보 생성

```
/iscsi/iqn.20.../tpg1/portals> **create ip_address=192.168.10.200 ip_port=3260**
Using default IP port 3260
Created network portal 192.168.10.200:3260.
```

생성한 전체 정보 확인

```
/iscsi/iqn.20.../tpg1/portals> **ls /**
o- / ............................................................................. [...]
  o- backstores .................................................................. [...]
  | o- block ...................................................... [Storage Objects: 1]
  | | o- 192.168.10.200.disk1 ................ [/dev/sdb (20.0GiB) write-thru activated]
  | |   o- alua ....................................................... [ALUA Groups: 1]
  | |     o- default_tg_pt_gp ........................... [ALUA state: Active/optimized]
  | o- fileio ..................................................... [Storage Objects: 0]
  | o- pscsi ...................................................... [Storage Objects: 0]
  | o- ramdisk .................................................... [Storage Objects: 0]
  o- iscsi ................................................................ [Targets: 1]
  | o- iqn.2020-07.com.example:192.168.10.200 ................................ [TPGs: 1]
  |   o- tpg1 ................................................... [no-gen-acls, no-auth]
  |     o- acls .............................................................. [ACLs: 1]
  |     | o- iqn.2020-07.com.example:192.168.10.230 ................... [Mapped LUNs: 1]
  |     |   o- mapped_lun0 ...................... [lun0 block/192.168.10.200.disk1 (rw)]
  |     o- luns .............................................................. [LUNs: 1]
  |     | o- lun0 ........... [block/192.168.10.200.disk1 (/dev/sdb) (default_tg_pt_gp)]
  |     o- portals ........................................................ [Portals: 1]
  |       o- 192.168.10.200:3260 .................................................. [OK]
  o- loopback ............................................................. [Targets: 0]
```

저장하고 빠져나가기

```
/iscsi/iqn.20.../tpg1/portals> **cd /**
/> **saveconfig** 
Last 10 configs saved in /etc/target/backup/.
Configuration saved to /etc/target/saveconfig.json
/> **exit**
Global pref auto_save_on_exit=true
Last 10 configs saved in /etc/target/backup/.
Configuration saved to /etc/target/saveconfig.json
```

**[SERVER1]# firewall-cmd --permanent --add-port=3260/tcp**

**[SERVER1]# firewall-cmd --reload**

**[SERVER1]# systemctl restart target**

**[SERVER1]# systemctl status target**

**[SERVER1]# systemctl enable target**

[SERVER1]# vi /etc/iscsi/initiatorname.iscsi

```bash
InitiatorName=iqn.2020-07.com.example:192.168.10.200
:wq
```

> iscsi 클라이언트 작업

**[SERVER4]# dnf search iscsi**

**[SERVER4]# dnf info iscsi-initiator-utils**

**[SERVER4]# dnf -y install iscsi-initiator-utils**

**[SERVER4]# rpm -ql iscsi-initiator-utils**

**[SERVER4]# vi /etc/iscsi/initiatorname.iscsi**

```
InitiatorName=iqn.2020-07.com.example:192.168.10.230
```

**[SERVER4]# systemctl restart iscsi**

**[SERVER4]# systemctl enable iscsi**

**[SERVER4]# iscsiadm -m discovery -t st -p 192.168.10.200**

```bash
192.168.10.200:3260,1 iqn.2020-07.com.example:192.168.10.200
```

**[SERVER4]# iscsiadm -m node -T iqn.2020-07.com.example:192.168.10.200 -p 192.168.10.200 -l**

```
Logging in to [iface: default, target: iqn.2020-07.com.example:192.168.10.200, portal: 192.168.10.200,3260]
Login to [iface: default, target: iqn.2020-07.com.example:192.168.10.200, portal: 192.168.10.200,3260] **successful.**
```

**[SERVER4]# lsblk**

```
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0  100G  0 disk 
├─sda1   8:1    0    1G  0 part /boot
├─sda2   8:2    0  7.9G  0 part [SWAP]
├─sda3   8:3    0    5G  0 part /home
├─sda4   8:4    0    1K  0 part 
└─sda5   8:5    0 86.1G  0 part /
**sdb      8:16   0   20G  0 disk** 
sr0     11:0    1 1024M  0 rom
```

**[SERVER4]# iscsiadm -m session -P 3**

```
iSCSI Transport Class version 2.0-870
version 6.2.0.878-2
Target: iqn.2020-07.com.example:192.168.10.200 (non-flash)
	Current Portal: 192.168.10.200:3260,1
	Persistent Portal: 192.168.10.200:3260,1
		**********
		Interface:
		**********
		Iface Name: default
		Iface Transport: tcp
		Iface Initiatorname: iqn.2020-07.com.example:192.168.10.230
		Iface IPaddress: 192.168.10.230
		Iface HWaddress: default
		Iface Netdev: default
		SID: 1
		iSCSI Connection State: LOGGED IN
		iSCSI Session State: LOGGED_IN
		Internal iscsid Session State: NO CHANGE
		*********
		Timeouts:
		*********
		Recovery Timeout: 120
		Target Reset Timeout: 30
		LUN Reset Timeout: 30
		Abort Timeout: 15
		*****
		CHAP:
		*****
		username: <empty>
		password: ********
		username_in: <empty>
		password_in: ********
		************************
		Negotiated iSCSI params:
		************************
		HeaderDigest: None
		DataDigest: None
		MaxRecvDataSegmentLength: 262144
		MaxXmitDataSegmentLength: 262144
		FirstBurstLength: 65536
		MaxBurstLength: 262144
		ImmediateData: Yes
		InitialR2T: Yes
		MaxOutstandingR2T: 1
		************************
		Attached SCSI devices:
		************************
		Host Number: 3	State: running
		scsi3 Channel 00 Id 0 Lun: 0
			Attached scsi disk sdb		State: running
```

**[SERVER4]# cd /var/lib/iscsi/nodes**

**[SERVER4]# ls**

```bash
iqn.2020-07.com.example:192.168.10.200
```

**[SERVER4]# iscsiadm -m node -T iqn.2020-07.com.example:192.168.10.200 -p 192.168.10.200 -u**

```
Logging out of session [sid: 1, target: iqn.2020-07.com.example:192.168.10.200, portal: 192.168.10.200,3260]
Logout of [sid: 1, target: iqn.2020-07.com.example:192.168.10.200, portal: 192.168.10.200,3260] successful.
```

**[SERVER4]# systemctl restart iscsi**

**[SERVER4]# lsblk**

```bash
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda           8:0    0  100G  0 disk 
├─sda1        8:1    0    1G  0 part /boot
└─sda2        8:2    0   99G  0 part 
  ├─cl-root 253:0    0 86.1G  0 lvm  /
  ├─cl-swap 253:1    0  7.9G  0 lvm  [SWAP]
  └─cl-home 253:2    0    5G  0 lvm  /home
sdb           8:16   0   10G  0 disk 
sr0          11:0    1 1024M  0 rom             << 여전히 존재
```

**[SERVER4]# iscsiadm -m node -T iqn.2020-07.com.example:192.168.10.200 -p 192.168.10.200 -u**

**[SERVER4]# iscsiadm -m node -T iqn.2020-07.com.example:192.168.10.200 -p 192.168.10.200 -o delete**

**[SERVER4]# lsblk**

```bash
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda           8:0    0  100G  0 disk 
├─sda1        8:1    0    1G  0 part /boot
└─sda2        8:2    0   99G  0 part 
  ├─cl-root 253:0    0 86.1G  0 lvm  /
  ├─cl-swap 253:1    0  7.9G  0 lvm  [SWAP]
  └─cl-home 253:2    0    5G  0 lvm  /home
sr0          11:0    1 1024M  0 rom
```

**[SERVER4]# ls -lR**

```
.:
합계 0
```

**[SERVER4]# systemctl restart iscsi**

**[SERVER4]# lsblk**

```
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0  100G  0 disk 
├─sda1   8:1    0    1G  0 part /boot
├─sda2   8:2    0  7.9G  0 part [SWAP]
├─sda3   8:3    0    5G  0 part /home
├─sda4   8:4    0    1K  0 part 
└─sda5   8:5    0 86.1G  0 part /
sr0     11:0    1 1024M  0 rom
```

**[SERVER4]# iscsiadm -m discovery -t st -p 192.168.10.200**

**[SERVER4]# ls -R**

```
.:
iqn.2020-07.com.example:192.168.10.200

'./iqn.2020-07.com.example:192.168.10.200':
192.168.10.200,3260,1

'./iqn.2020-07.com.example:192.168.10.200/192.168.10.200,3260,1':
default
```

**[SERVER4]# iscsiadm -m node -T iqn.2020-07.com.example:192.168.10.200 -p 192.168.10.200 -o delete**

### 정리

```bash
검색 : /var/lib/iscsi/nodes > 등록
login > 블록 디바이스 사용: 장치관리
-u 잠시 해제
-o delete 장치를 /var/lib/iscsi/nodes , lsblk 삭제
```

> 마운트 작업

[SERVER4]# lsblk

```
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0  100G  0 disk 
├─sda1   8:1    0    1G  0 part /boot
├─sda2   8:2    0  7.9G  0 part [SWAP]
├─sda3   8:3    0    5G  0 part /home
├─sda4   8:4    0    1K  0 part 
└─sda5   8:5    0 86.1G  0 part /
**sdb      8:16   0   20G  0 disk** 
sr0     11:0    1 1024M  0 rom
```

**[SERVER4]# fdisk /dev/sdb**

```
Welcome to fdisk (util-linux 2.32.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x42de8a41.

Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): 

Using default response p.
Partition number (1-4, default 1): 
First sector (8192-41943039, default 8192): 
Last sector, +sectors or +size{K,M,G,T,P} (8192-41943039, default 41943039): 

Created a new partition 1 of type 'Linux' and of size 20 GiB.

Command (m for help): p
Disk /dev/sdb: 20 GiB, 21474836480 bytes, 41943040 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 4194304 bytes
Disklabel type: dos
Disk identifier: 0x42de8a41

Device     Boot Start      End  Sectors Size Id Type
/dev/sdb1        8192 41943039 41934848  20G 83 Linux

Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

```

**[SERVER4]# mkfs.xfs /dev/sdb1**

```
meta-data=/dev/sdb1              isize=512    agcount=4, agsize=1310464 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=5241856, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
```

**[SERVER4]# mkdir /stage**

**[SERVER4]# mount /dev/sdb1 /stage/**

[SERVER4]# df .

```
Filesystem     1K-blocks   Used Available Use% Mounted on
/dev/sdb1       20957184 179176  20778008   1% /stage
```

> [참고] 영구적 마운트를 하는 경우 주의점

보통 마운트가 네트워크 연결보다 먼저 진행되기 때문에 기본적인 구성으로는 에러가 발생한다. 
iscsi 장치를 fstab에 등록하는 경우에는 네트워크 디바이스라는 의미로 _netdev를 추가해 주어야 한다. 

네트워크 장치라고 표시해 준다면 네트워크 로딩이후 마운트를 재시도한다.

```
#
# /etc/fstab
# Created by anaconda on Tue Jul 14 08:35:53 2020
#
# Accessible filesystems, by reference, are maintained under '/dev/disk/'.
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info.
#
# After editing this file, run 'systemctl daemon-reload' to update systemd
# units generated from this file.
#
UUID=dd42ca00-ec0c-4659-b096-ae21639703d4 /                       xfs     defaults        0 0
UUID=dfe1e4b1-295c-4344-9781-291f7d9478e7 /boot                   ext4    defaults        1 2
UUID=d0b93436-b57f-4b3c-ad6e-e2becd3dbb3e /home                   xfs     defaults        0 0
UUID=9bc56f33-11ab-46fb-8096-78a14f302fdc swap                    swap    defaults        0 0
UUID=e5894907-a4f6-49ab-806f-8df317934fa1 /stage                  xfs     defaults,_netdev        0 0
```

# 삭제는 역순으로

[SERVER4]# umount /stage

[SERVER4]# vi /etc/fstab

```bash
/stage 라인 삭제
```

**[SERVER4]# iscsiadm -m node -T iqn.2020-07.com.example:192.168.10.200 -p 192.168.10.200 -u**

**[SERVER4]# iscsiadm -m node -T iqn.2020-07.com.example:192.168.10.200 -p 192.168.10.200 -o delete**

[SERVER1]# targetcli

```bash
/> cd /iscsi/iqn.2020-07.com.example:192.168.10.200/tpg1/portal
/> delete
/> cd /iscsi/iqn.2020-07.com.example:192.168.10.200/tpg1/luns/
/> delete
/> cd /iscsi/iqn.2020-07.com.example:192.168.10.200
/> delete
/> cd /iscsi
/> delete iqn.~
/> cd /
/> clearconfig confirm=True
/> ls /
/> saveconfig
/> exit
```

[SERVER1]# lsblk

```bash
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda           8:0    0  100G  0 disk 
├─sda1        8:1    0    1G  0 part /boot
└─sda2        8:2    0   99G  0 part 
  ├─cl-root 253:0    0 86.1G  0 lvm  /
  ├─cl-swap 253:1    0  7.9G  0 lvm  [SWAP]
  └─cl-home 253:2    0    5G  0 lvm  /home
sdb           8:16   0   10G  0 disk 
sdc           8:32   0   10G  0 disk 
sr0          11:0    1 1024M  0 rom
```

[SERVER1]# dd if=/dev/zero of=/dev/sdb

시간이 좀 걸린다.

```bash
dd: writing to '/dev/sdb': No space left on device
20971521+0 records in
20971520+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 118.71 s, 90.5 MB/s
```

# STRATIS

**Stratis 로컬 스토리지 관리를 사용하여 여러 스토리지 계층을 관리**

## STRATIS 아키텍처

RHEL의 최신 로컬 스토리지 솔루션에는 DM(장치 매퍼), LVM(논리 볼륨 관리자), XFS 파일 시스템을 비롯하여 안정적이고 성숙한 기술이 다양하게 포함되어 있습니다. 이러한 구성 요소가 제공하는 기능에는 대규모로 확장 가능한 파일 시스템, 스냅샷, 중복(RAID) 논리 장치, 다중 경로 지정, 씬 프로 비저닝, 캐싱, 중복 제거, 가상 시스템 및 컨테이너 지원이 포함됩니다. 

각 스토리지 스택 계층(dm, LVM, XFS)은 계층별 명령 및 유틸리티를 사용하여 관리되며, 시스템 관리자가 물리 장치, 크기가 고정된 볼륨, 파일 시스템을 별도의 스토리지 구성 요소로 관리해야 합니다.

기존에 사용하는 파일시스템 및 디바이스 관리에 대한 여러 유틸리티 사용, 명령어의 혼합, 등등..이 존재하기 때문에 시스템 관리자가 해야 할 일이 많다. 

```
디스크를 사용하기 위해 지금까지 한 작업

디스크 장착
디스크 인식
lspci -tv
scan 확인
echo "- - -" > scan
lsscsi
fdisk
mkfs
mkdir
mount
fstab
```

최근 몇 년간은 볼륨 관리 파일 시스템이라는 차세대 스토리지 관리 솔루션이 등장했습니다. 이 시스템은 파 일 시스템을 생성하고 크기를 조정할 때 볼륨 계층을 동적으로 투명하게 관리합니다. 그러나 이러한 파일 시 스템의 커뮤니티 개발은 수년 간 진행되었지만 Red Hat Enterprise Linux의 기본 로컬 스토리지를 관리하는 데 필요한 기능 지원 및 안정성 수준에 도달하지 못했습니다.

RHEL 8, Red Hat에는 Stratis 스토리지 관리 솔루션이 도입되었습니다. Stratis는 다른 스토리지 프로젝트(gluster, gfs, chep.. )에서 시도한 것처럼 처음부터 개발하지 않고 기존 RHEL 스토리지 구성 요소를 사용하여 작동합니다. Stratis는 `물리적 스토리지 장치 풀을 관리하는 서비스로 실행`되며, 생성되는 파일 시스템의 볼륨을 투명하게 생성 및 관리합니다. Stratis는 기존 스토리지 드라이버 및 도구를 사용하기 때문에 현재 LVM, XFS, 장치 매퍼에서 사 용하는 모든 고급 스토리지 기능이 Stratis에서도 지원됩니다.

볼륨 관리 파일 시스템에서 파일 시스템은 **씬 프로비저닝**이라는 개념을 사용하여 디스크 장치의 공유 풀 내부 에 빌드됩니다. Stratis 파일 시스템은 크기가 고정되어 있지 않으며 사용되지 않는 블록 공간을 더 이상 사전 할당하지 않습니다. 파일 시스템은 여전히 숨겨진 LVM 볼륨에 빌드되지만 Stratis에서 기본 볼륨을 관리하고 필요한 경우 확장할 수 있습니다. 사용 중인 파일 시스템 크기는 포함된 파일에서 사용 중인 실제 블록 양으로 표시됩니다. 파일 시스템에서 사용할 수 있는 공간은 파일 시스템이 상주하는 풀링된 장치에서 아직 사용되지 않은 공간의 양입니다. 여러 파일 시스템이 동일한 디스크 장치 풀에 상주하며 사용 가능한 공간을 공유할 수 있지만 필요한 경우 가용성을 보장하기 위해 파일 시스템에서 풀 공간을 예약할 수도 있습니다.

```
가상화에서 시작된 개념

thin 프로비저닝
모든 용량을 사용하지 않고 사용되는 만큼 조금씩 디스크의 크기를 점유하는 방식

느리게 비워지는 프로비저닝
생성은 빠르다. 하지만 실제 사용을 해야 하는 경우 약간 느리다.
디스크 원본 파일만 생성-> 내부를 0으로 초기화-> 파일 write

빠르게 비워지는 프로비저닝
생성은 느리다. 하지만 실제 사용을 해야 하는 경우 빠르다.
디스크 원본 파일을 생성할 때 배누를 0으로 초기화 -> 이미 모든 초기화 작업이 끝나있기에 파일을 write하는 경우 빠르게 가능하다. 

thick 프로비저닝
모든 용량을 사용하여 디스크의 크기를 점유하는 방식

```

Stratis는 저장된 메타데이터를 사용하여 관리되는 풀, 볼륨 및 파일 시스템을 인식합니다. 따라서 Stratis가 만든 파일 시스템은 수동으로 다시 포맷하거나 재구성해서는 안 됩니다. Stratis 도구 및 명령으로만 관리해야 합니다. Stratis 파일 시스템을 수동으로 구성하면 해당 메타데이터가 손실되어 Stratis가 생성한 파일 시스템 을 인식하지 못할 수 있습니다.

다양한 블록 장치 집합을 사용하여 여러 개의 풀을 만들 수 있습니다. 각 풀에서 하나 이상의 파일 시스템을 만 들 수 있습니다. 현재 풀당 최대 224 개의 파일 시스템을 만들 수 있습니다. 다음 다이어그램은 Stratis 스토리 지 관리 솔루션의 요소가 배치되는 방법을 보여줍니다.

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4604faca-0ef0-4925-aa33-c7fcc4e81a08/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4604faca-0ef0-4925-aa33-c7fcc4e81a08/Untitled.png)

풀은 블록 장치를 데이터 계층 및 선택적으로 캐시 계층으로 그룹화합니다. 데이터 계층은 유연성과 무결성에 중점을 두고 캐시 계층은 성능향상에 중점을 둡니다. 캐시 계층은 성능향상을 위한것 이므로 SSD와 같이 초당 입/출력(IOPS)이 높은 블록 장치를 사용해야 합니다.

## 간소화된 스토리지 스택 설명

Stratis는 다양한 Red Hat 제품에서 로컬 스토리지 프로비저닝 및 구성의 여러 측면을 간소화합니다. 예를 들 어, 이전 버전의 Anaconda 설치 프로그램에서는 시스템 관리자가 디스크 관리의 각 측면을 다른 측면과 분리해야 했습니다. 이제 설치 프로그램에서 Stratis를 사용하여 디스크 설정을 간소화합니다. Stratis를 사용 하는 기타 제품에는 Cockpit, Red Hat Virtualization, Red Hat Enterprise Linux Atomic Host가 포함됩니다. 이러한 모든 제품의 경우 Stratis를 통해 스토리지 공간 및 스냅샷 관리가 간소화되고 오류가 줄어듭니다. Stratis를 사용하면 프로그래밍 방식으로 CLI를 사용하는 것보다 고급 관리 도구에 쉽게 통합할 수 있습니다.

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e3fb82cf-aa4a-4c8e-b950-6de224d8d72b/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e3fb82cf-aa4a-4c8e-b950-6de224d8d72b/Untitled.png)

Linux 스토리지 관리 스택의 Stratis

## Stratis 계층 설명

내부적으로 Stratis는 Backstore 하위 시스템을 사용하여 블록 장치를 관리하고, Thinpool 하위 시스템 으로 풀을 관리합니다. Backstore 하위 시스템에는 블록 장치의 디스크 메타데이터를 유지 관리하고 데이 터 손상을 탐지 및 수정하는 데이터 계층이 있습니다. 캐시 계층은 고성능 블록 장치를 사용하여 데이터 계층 위에서 캐시 역할을 수행합니다. Thinpool 하위 시스템은 Stratis 파일 시스템과 연결된 씬 프로비저닝 볼 륨을 관리합니다. 이 하위 시스템은 가상 볼륨의 크기를 조정하고 관리할 때 dm-thin 장치 매퍼 드라이버를 사용하여 LVM을 교체합니다. dm-thin은 가상 크기가 크고 XFS로 포맷되지만 물리적 크기는 작은 볼륨을 만듭니다. 물리적 크기가 거의 가득 차면 Stratis에서 자동으로 확대합니다.

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3f10986d-1b56-4ce6-bd5c-209492d47e7a/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3f10986d-1b56-4ce6-bd5c-209492d47e7a/Untitled.png)

## 씬 프로비저닝된 파일 시스템 관리

Stratis 스토리지 관리 솔루션을 사용하여 씬 프로비저닝된 파일 시스템을 관리하려면 stratis-cli 및 stratisd 패키지를 설치하십시오. stratis-cli 패키지는 사용자 요청을 D-Bus API를 통해 stratisd 서비스로 변환하 는 stratis 명령을 제공합니다. stratisd 패키지는 D-Bus 인터페이스를 구현하고 Stratis의 요소(예: 블록 장치, 풀, 파일 시스템)를 관리 및 모니터링하는 stratisd 서비스를 제공합니다. stratisd 서비스가 실 행 중인 경우 D-Bus API를 사용할 수 있습니다.

[root@server1 ~]# dnf search stratis

```
Last metadata expiration check: 1:41:00 ago on Thu 21 Jan 2021 12:59:03 PM KST.
============= Name & Summary Matched: stratis ==============
stratis-cli.noarch : Command-line tool for interacting with
                   : the Stratis daemon
================== Name Matched: stratis ===================
stratisd.x86_64 : Daemon that manages block devices to
                : create filesystems
```

[root@server1 ~]# dnf -y install stratis-cli stratisd

```
Last metadata expiration check: 1:41:56 ago on Thu 21 Jan 2021 12:59:03 PM KST.
Dependencies resolved.
============================================================
 Package                 Arch   Version     Repo       Size
============================================================
Installing:
 stratis-cli             noarch 2.1.1-6.el8 appstream  75 k
 stratisd                x86_64 2.1.0-1.el8 appstream 1.4 M
Installing dependencies:
 python3-dbus-client-gen noarch 0.4-1.el8   appstream  26 k
 python3-dbus-python-client-gen
                         noarch 0.7-3.el8   appstream  28 k
 python3-dbus-signature-pyparsing
                         noarch 0.03-2.el8  appstream  18 k
 python3-into-dbus-python
                         noarch 0.06-2.el8  appstream  27 k
 python3-justbases       noarch 0.14-4.el8  appstream  46 k
 python3-justbytes       noarch 0.14-2.el8  appstream  43 k
 python3-psutil          x86_64 5.7.3-1.el8 epel      421 k
 python3-semantic_version
                         noarch 2.6.0-5.el8 appstream  30 k

Transaction Summary
============================================================
Install  10 Packages

Total download size: 2.1 M
Installed size: 7.4 M
Downloading Packages:
(1/10): python3-dbus-signat  21 kB/s |  18 kB     00:00    
(2/10): python3-dbus-client  29 kB/s |  26 kB     00:00    
(3/10): python3-dbus-python  31 kB/s |  28 kB     00:00    
(4/10): python3-into-dbus-p 187 kB/s |  27 kB     00:00    
(5/10): python3-justbases-0 159 kB/s |  46 kB     00:00    
(6/10): python3-semantic_ve 204 kB/s |  30 kB     00:00    
(7/10): python3-justbytes-0 142 kB/s |  43 kB     00:00    
(8/10): stratis-cli-2.1.1-6 493 kB/s |  75 kB     00:00    
(9/10): python3-psutil-5.7. 695 kB/s | 421 kB     00:00    
(10/10): stratisd-2.1.0-1.e 1.4 MB/s | 1.4 MB     00:01    
------------------------------------------------------------
Total                       334 kB/s | 2.1 MB     00:06     
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                    1/1 
  Installing       : python3-psutil-5.7.3-1.el8.x86    1/10 
  Installing       : stratisd-2.1.0-1.el8.x86_64       2/10 
  Running scriptlet: stratisd-2.1.0-1.el8.x86_64       2/10 
  Installing       : python3-semantic_version-2.6.0    3/10 
  Installing       : python3-justbases-0.14-4.el8.n    4/10 
  Installing       : python3-justbytes-0.14-2.el8.n    5/10 
  Installing       : python3-dbus-signature-pyparsi    6/10 
  Installing       : python3-into-dbus-python-0.06-    7/10 
  Installing       : python3-dbus-python-client-gen    8/10 
  Installing       : python3-dbus-client-gen-0.4-1.    9/10 
  Installing       : stratis-cli-2.1.1-6.el8.noarch   10/10 
  Running scriptlet: stratis-cli-2.1.1-6.el8.noarch   10/10 
  Verifying        : python3-dbus-client-gen-0.4-1.    1/10 
  Verifying        : python3-dbus-python-client-gen    2/10 
  Verifying        : python3-dbus-signature-pyparsi    3/10 
  Verifying        : python3-into-dbus-python-0.06-    4/10 
  Verifying        : python3-justbases-0.14-4.el8.n    5/10 
  Verifying        : python3-justbytes-0.14-2.el8.n    6/10 
  Verifying        : python3-semantic_version-2.6.0    7/10 
  Verifying        : stratis-cli-2.1.1-6.el8.noarch    8/10 
  Verifying        : stratisd-2.1.0-1.el8.x86_64       9/10 
  Verifying        : python3-psutil-5.7.3-1.el8.x86   10/10 
Installed products updated.

Installed:
  python3-dbus-client-gen-0.4-1.el8.noarch                  
  python3-dbus-python-client-gen-0.7-3.el8.noarch           
  python3-dbus-signature-pyparsing-0.03-2.el8.noarch        
  python3-into-dbus-python-0.06-2.el8.noarch                
  python3-justbases-0.14-4.el8.noarch                       
  python3-justbytes-0.14-2.el8.noarch                       
  python3-psutil-5.7.3-1.el8.x86_64                         
  python3-semantic_version-2.6.0-5.el8.noarch               
  stratis-cli-2.1.1-6.el8.noarch                            
  stratisd-2.1.0-1.el8.x86_64                               

Complete!
```

[root@server1 ~]# rpm -ql stratisd

[root@server1 ~]# rpm -ql stratis-cli

[root@server1 ~]# systemctl enable stratisd

[root@server1 ~]# systemctl start stratisd

[root@server1 ~]# lsblk

```
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda           8:0    0  100G  0 disk 
├─sda1        8:1    0    1G  0 part /boot
└─sda2        8:2    0   99G  0 part 
  ├─cl-root 253:0    0 86.1G  0 lvm  /
  ├─cl-swap 253:1    0  7.9G  0 lvm  [SWAP]
  └─cl-home 253:2    0    5G  0 lvm  /home
**sdb           8:16   0   10G  0 disk 
sdc           8:32   0   10G  0 disk** 
sr0          11:0    1 1024M  0 rom
```

현재 생성되어 있는 pool을 확인한다. 하지만 설치 직후이기 때문에 어떠한 pool도 존재하지 않는다.

[root@server1 ~]# stratis pool list

```
Name   Total Physical   Properties
```

pool을 생성한다.

[root@server1 ~]# stratis pool create stratispool1 /dev/sdb

```
stratis      stratis 서비스에서 관리되는 여러 장치들을 관리하기 위한 명령어
pool         블록디바이스의 집합
create       생성
stratispool1 생성되는 pool의 이름
/dev/sdb     생성당시 pool에 추가할 디바이스
```

[root@server1 ~]# stratis pool list

```
Name                          Total Physical   Properties
stratispool1   10 GiB / 37.63 MiB / 9.96 GiB      ~Ca,~Cr
```

하나의 디스크로 사용되고 있는 pool에 추가 디스크를 장착한다.

[root@server1 ~]# stratis pool add-data stratispool1 /dev/sdc

[root@server1 ~]# stratis pool list

```
Name                           Total Physical   Properties
stratispool1   20 GiB / 41.64 MiB / 19.96 GiB      ~Ca,~Cr
```

pool에 추가한 디스크를 확인해보자.

[root@server1 ~]# stratis blockdev list stratispool1

```
Pool Name      Device Node   Physical Size   Tier
stratispool1   /dev/sdb             10 GiB   Data
stratispool1   /dev/sdc             10 GiB   Data
```

씬 프로비저닝 파일시스템 생성

[root@server1 ~]# stratis filesystem create stratispool1 stratis-filesystem1

```
stratis 
filesystem 파일시스템 관련 작업
create 파일시스템 생성
stratispool1 어느 pool에서 파일 시스템을 생성할 것인지 선택
stratis-filesystem1 생성될 파일 시스템 이름
```

[root@server1 ~]# stratis filesystem list stratispool1

```
Pool Name      Name                  Used      Created             Device                                      UUID                            
stratispool1   stratis-filesystem1   546 MiB   Jan 21 2021 14:57   /stratis/stratispool1/stratis-filesystem1   6afdfa876c9647eda6a4efea459960b6
```

파일시스템까지 생성되었으며 이제 사용을 위하여 마운트 작업을 실시한다.

[root@server1 ~]# mkdir /stratisvol

[root@server1 ~]# mount /stratis/stratispool1/stratis-filesystem1 /stratisvol/

[root@server1 ~]# cd /stratisvol/

[root@server1 stratisvol]# df .

[root@server1 stratisvol]# df -T

[root@server1 stratisvol]# mount | grep stratisvol

[root@server1 stratisvol]# touch file1

[root@server1 stratisvol]# ls -l

임의의 파일을 생성하고 파티션의 용량을 다시 한번 체크해보자.

[root@server1 stratisvol]# dd if=/dev/urandom of=/stratisvol/file2 bs=1M count=2048

[root@server1 stratisvol]# stratis filesystem list

```
Pool Name      Name                  Used       Created             Device                                      UUID                            
stratispool1   stratis-filesystem1   2.53 GiB   Jan 21 2021 14:57   /stratis/stratispool1/stratis-filesystem1   6afdfa876c9647eda6a4efea459960b6
```

스냅샷을 생성하고 스냅샷의 파일로 복구를 진행 해보자.

현재 시점의 상태로 스냅샷을 구성한다.

[root@server1 stratisvol]# stratis filesystem snapshot stratispool1 stratis-filesystem1 stratis-filesystem1-snap1-2020-01-21

[root@server1 stratisvol]# stratis filesystem list

```
Pool Name      Name                                   Used       Created             Device                                                       UUID                            
stratispool1   stratis-filesystem1                    2.54 GiB   Jan 21 2021 14:57   /stratis/stratispool1/stratis-filesystem1                    6afdfa876c9647eda6a4efea459960b6
stratispool1   stratis-filesystem1-snap1-2020-01-21   2.54 GiB   Jan 21 2021 15:14   /stratis/stratispool1/stratis-filesystem1-snap1-2020-01-21   6a34c1d951ac4e039cb068e723e70327

스냅샷은 따로 명령어 세트가 있는것이 아니기에 filesystem쪽에서 확인해야 한다.
```

기존의 file2를 제거한다.

[root@server1 stratisvol]# pwd

```
/stratisvol
```

[root@server1 stratisvol]# rm -rf file2

[root@server1 stratisvol]# ls

```
file1  file3
```

스냅샷 이지만 파일시스템이기 때문에 마운트이후 파일을 복원할 수 있다.

[root@server1 stratisvol]# mkdir /stratis-snap

[root@server1 stratisvol]# mount /stratis/stratispool1/stratis-filesystem1-snap1-2020-01-21 /stratis-snap

[root@server1 stratisvol]# cd /stratis-snap

[root@server1 stratis-snap]# ls -l

```
total 2107392
-rw-r--r--. 1 root root          0 Jan 21 15:03 file1
-rw-r--r--. 1 root root 2147483648 Jan 21 15:08 file2
-rw-r--r--. 1 root root   10485760 Jan 21 15:08 file3

삭제된 file2가 존재한다.
```

[root@server1 stratis-snap]# cp -p file2 /stratisvol/

[root@server1 stratis-snap]# cd /stratisvol/

[root@server1 stratisvol]# ls

```
file1  file2  file3
기존 파일을 복구 하였다.
```

snap을 생성한 이후 계속 보관하게되면 무한정으로쌓이기에 해당 pool의 점유율이 올라가 실제 사용해야 하는 용량이 줄어든다. 그렇기에 과거의 snapshot 파일은 제거를 해주어야 한다.

[root@server1 stratisvol]# cd

[root@server1 ~]# umount /stratis-snap

[root@server1 ~]# stratis filesystem destroy stratispool1 stratis-filesystem1-snap1-2020-01-21

[root@server1 ~]# stratis filesystem list

```
Pool Name      Name                  Used       Created             Device                                      UUID                            
stratispool1   stratis-filesystem1   2.67 GiB   Jan 21 2021 14:57   /stratis/stratispool1/stratis-filesystem1   6afdfa876c9647eda6a4efea459960b6
```

기존 파일시스템 , pool 삭제하기

[root@server1 ~]# stratis filesystem list

```
Pool Name      Name                  Used       Created             Device                                      UUID                            
stratispool1   stratis-filesystem1   2.67 GiB   Jan 21 2021 14:57   /stratis/stratispool1/stratis-filesystem1   6afdfa876c9647eda6a4efea459960b6
```

현재 마운트 되어 있는 /stratisvol을 해제

[root@server1 ~]# df

```
Filesystem                                                                                       1K-blocks    Used  Available Use% Mounted on
devtmpfs                                                                                            452576       0     452576   0% /dev
tmpfs                                                                                               482008       0     482008   0% /dev/shm
tmpfs                                                                                               482008    7228     474780   2% /run
tmpfs                                                                                               482008       0     482008   0% /sys/fs/cgroup
/dev/mapper/cl-root                                                                               90260420 7298164   82962256   9% /
/dev/mapper/cl-home                                                                                5232640   69652    5162988   2% /home
/dev/sda1                                                                                          1038336  289752     748584  28% /boot
tmpfs                                                                                                96400       0      96400   0% /run/user/0
/dev/mapper/stratis-1-00c7eb3ff1c74d1cb5365bc40bd74c84-thin-fs-6afdfa876c9647eda6a4efea459960b6 1073217536 9624912 1063592624   1% /stratisvol
```

[root@server1 ~]# umount /stratisvol
[root@server1 ~]# stratis filesystem destroy stratispool1 stratis-filesystem1

[root@server1 ~]# stratis pool destroy stratispool1

[root@server1 ~]# stratis blockdev

```
Pool Name   Device Node   Physical Size   Tier
```

[root@server1 ~]# dd if=/dev/zero of=/dev/sdb bs=1M count=20

[root@server1 ~]# dd if=/dev/zero of=/dev/sdc bs=1M count=20

## 영구마운트 하기

/etc/fstab 파일에 추가해 주어야 한다.

하지만 일반적인 상황으로 추가를 하게 되면 ERROR를 일으킨다.

[root@server1 ~]# man systemd.mount

```
x-systemd.requires=

부팅시에 mount가 먼저 진행되고 그 이후에 서비스가 기동되는데 기본 defaults값만 주고서 부팅을 진행하게 되면 서비스가(stratisd)가 기동되지 않은 상태이기 때문에 마운트가 정상적으로 이루어지지 않고 재부팅후 emergency.target으로 진행된다. 이를 막기위해 x-systemd.requires=stratisd.service를 입력해주면 해당 서비스(stratisd.service)를 실행한 이후에 마운트하도록 지연시킨다.
```

[root@server1 ~]# lsblk --output=UUID /stratis/stratispool1/stratis-filesystem1

```
UUID
7ea7fce9-fe31-491b-a116-df397686caa9
```

/stratis/stratispool1/stratis-filesystem1

[root@server1 ~]# vi /etc/fstab

```
UUID=7ea7fce9-fe31-491b-a116-df397686caa9       /stratisvol     xfs     defaults,x-systemd.requires=stratisd.service 0 0
```
